{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DataSavvyYT/experiments/blob/main/0_setup/create_unsloth_env_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kui4StwKtejj",
    "outputId": "3867872b-d072-4dc9-e260-a77227e5552a"
   },
   "outputs": [],
   "source": [
    "!pip install -q virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XTZFA43tfHw",
    "outputId": "ea677de7-6256-4577-d300-f3840cf08ca2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKlCDcGatrLS"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/drive/MyDrive/unsloth_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPhBNgsjuBix",
    "outputId": "daa75a1b-151d-4bc4-a075-e1d04ed23df8"
   },
   "outputs": [],
   "source": [
    "# Create the environment folder in Drive\n",
    "!virtualenv --system-site-packages /content/drive/MyDrive/unsloth_env\n",
    "\n",
    "# Add the new environment's site-packages to the current Python path\n",
    "import sys\n",
    "sys.path.append(\"/content/drive/MyDrive/unsloth_env/lib/python3.11/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qReFMuyJuEAq"
   },
   "outputs": [],
   "source": [
    "# Installs Unsloth, Xformers (for speed), and TRL\n",
    "%%capture\n",
    "!/content/drive/MyDrive/unsloth_env/bin/pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!/content/drive/MyDrive/unsloth_env/bin/pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrwTOR4Eu8_A"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Update 'python3.x' to match the current Colab version (usually python3.11 or python3.12 in late 2025)\n",
    "env_path = \"/content/drive/MyDrive/unsloth_env/lib/python3.12/site-packages\"\n",
    "\n",
    "if env_path not in sys.path:\n",
    "    sys.path.append(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCdSe9jzuG7s"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84409acf",
    "outputId": "0664bd8d-17ed-4ef1-9eef-8fe93112c2c0"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "d250b1c767ef47b3953f650976ff8555",
      "3a96ed6e44484bcd9bf7e0541ea851f5",
      "a4aa33b801ac48e2a849bdfdea81c55d",
      "b27b0f777c3844aab0a49f0e62120507",
      "857d3455655845889f53ab6d4a63a836",
      "3a03d26a07ed4aeb9c4cc4ac0a01f7c7",
      "a655e606e9014a58b4f876ce95dc7bb9",
      "7821447dc6e84bf0a31a7556deb183f8",
      "3f9ae12ef0cb4a67946e5d5d61d3bce4",
      "f3f927b9f83e4096be1b9e2456ce001e",
      "0f900c0103c24769a62e3c3a319d3d05",
      "34938f191eb545bfbdade7309e3b9580",
      "4665023570df406b8d16817c83f77817",
      "0e916e8967c148018af2880406e7269d",
      "3e204d711a764db49795c9a6af028a65",
      "2082e13c36114805ac15ab3f78c973a3",
      "4c7297cf332342929b241dd794197d0e",
      "311ccedd1a8447a5bf81c8874fdff202",
      "3aaf3a5f2410462197c49f45efe4c91a",
      "228c719613b54cd0a13686f75dcae7cc",
      "7d2e13d7adb04966b0cc2cd98d59e5f8",
      "ab815be0cb1740138e503a29f5579b96",
      "c772b5f8bb194df2bcd031c25767d54f",
      "51fb818969194ae5bd997944b2ebe096",
      "635b408ca42640468fc8a52cafeadb67",
      "60a0315d6cff4c61bf59930b497edd6c",
      "e9fbd2b9c8e24884b78df9708212b4c1",
      "f144852fc22440f88c31212c8b3e451d",
      "7f10e343907946919a7ed3eb795a510d",
      "b34628a548f34eae8143161de2f423e3",
      "8ad3684d21884c7aa7a87d2d5ea7a1cd",
      "606096487946406d99883d10f1816e72",
      "03ecd38c4aec4828b6a1df774d8581d2",
      "66f09aa0444844fabd8602b2cb60cc99",
      "4cc2fb636ce44469bdf4aed72a69b4bf",
      "788266556d494c168e6cfa35788a1152",
      "67dcab3ec02f403bb22491bd74df3ddb",
      "1e0b8b9ea71f4f6e9a6c8602d144dcaf",
      "10ea09be8ab54c0eac097d0f03bc818c",
      "ddab11c698954c529c6e7ccc194670eb",
      "92ce914d9caa497e8e62a6cf68fcae26",
      "c6da549c42c94b16a0f257c57044c7d0",
      "dd5e8b1d10b1493fb9a23222e497239f",
      "ccf95eaa907446649bff2b0345ed9080",
      "020b6129cc90483fa7b2fc8f62777822",
      "6332dd08ac674702af9531b7b6040efd",
      "d6d4a1ca92c84d5aad325e3f14f8bee7",
      "860129d70de74f9dbc1e6a5a9a3025ba",
      "256f056df7d2489d9f6d1a5f708209b8",
      "e0986792cbed4fbbb1429317f2a9dca1",
      "1fe1e9e15fff4524b128b0a54224ca24",
      "79537c0a71074bce9b47b31f902e8efa",
      "ee1486d8c83b4597a7e7257c9ada48a7",
      "94f4c221b93a4c1a9c4770656886e156",
      "80798392ce89410f81a799bedd96579d",
      "3670b45b4df748df9d7222e8f850afa8",
      "d23cc8218da240e3be0867d51bd69fed",
      "efcb4362f26b4736a3cc4985ab6338f3",
      "80e083fdd88842a18df9796d6cd76769",
      "9eea55db02b34d8abae70fee546c0b75",
      "e0bc4188428d47168dccee162b219a77",
      "dd85d8590e6a4c5ba68f48e7cbd5b7c5",
      "c5e74a187f5e4d3db0fa49bb4ae57a5b",
      "d2c33eaa8a4a4896bc4885b653da756c",
      "896e3b0233214bf78e85ed0cc60e4e01",
      "81d52cce9eac482d9673862715b57832"
     ]
    },
    "id": "nVEfASxauojM",
    "outputId": "d47c6fae-668a-4347-c363-3a48e4c089a5"
   },
   "outputs": [],
   "source": [
    "# 1. Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b\", # You can swap this for 'unsloth/mistral-7b'\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5_A0tVwurwI",
    "outputId": "6f81a7bb-bd57-4e53-a792-3a3d210fd2b3"
   },
   "outputs": [],
   "source": [
    "# 2. Add LoRA adapters (The \"Sticky Notes\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "eb5de981cc4148b1a3d6ee33996e874f",
      "d5530beb25544d26bbc4cc9640f5fa31",
      "004672cf95aa4cdba1bc4ab47fcc9dca",
      "391cbc04b85644aa80f78991c7df13df",
      "d171c0d1bd304485a79fd96a09b59afb",
      "9783ec7563df4a498d969fe6bf99b4a1",
      "be3f89512b574c008417960736348ff0",
      "0c582bd4c4c646f882e0db6afdfc71ca",
      "74c5fa620fb6425cb5eaedabcf75f7e4",
      "b86e947163004db2977e3865912d40dd",
      "821464611eb04685bb48a60555a3f3cb",
      "d2d1b1e696714deb851bda16b29cae41",
      "3aff61e97bec404393ba299336d61a29",
      "1554611e515b4a5d952b2d239a76cb5d",
      "906ed28321754a22ab459b29105d5473",
      "1013e37bb6434538aeb7d2d142ca8023",
      "52157ae7f5324102a2c10295871bb9c2",
      "00e91772a3da4017be2b124a286b783c",
      "bcf26a1597cd45cb955f4605729dd791",
      "257c3bf3baae4cc4a2adaffda7c56992",
      "04737ad70c9b4cc8925984716ce8a9d9",
      "b46a4702bafb4e41bd922a3388f3cffb",
      "3060d61f4ff64325b696395fa8003517",
      "1c45d2c806ec4cfa8bd7038bfc378014",
      "e313d07503b84a79bb88f0d2e1eadab0",
      "65f5d3fc8f934f8aaf2dd2dd14bb82e0",
      "df36798e5ea74f42938b25f441c6b8d4",
      "136377a987c641eeb3d9883458f78192",
      "1e779218d02644338623f3b0464c2984",
      "3625564192bf46e7af09c1d20e1cd92c",
      "297157f239c54034839076f19da046b6",
      "d259761a21f847be8a47b826839d5841",
      "2bbac8192ea34857aeec7d0c8bcd8691",
      "1b891a1c7ecd400f9afa9d1becef85d4",
      "dbed48c77d9f458cbce6c3034722ade1",
      "082404575ddb4881a5e24641b5411014",
      "ce63fe0c1a2e41ba9ca75313522799ef",
      "7dc19f85042c4ed3aa9791b01a127aef",
      "ca573a0a06df4bf483da5d7dd3aca761",
      "53e995468ee940338a2233288a2cd9ba",
      "229ff111802346b8b418773794b019c7",
      "21d8628aa0fd42fc9ba30234e02efc1a",
      "e960f3732b0848b58c50728e2d53319e",
      "95dca52af4454287a2fffc4058c74056"
     ]
    },
    "id": "fBoSARi2wHPf",
    "outputId": "099d81a7-ebde-455a-a5ff-786e4a350dfb"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 3. Load Dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "\n",
    "# 4. Format the prompts\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "86da25b3a9ea4b4c81c5e17f322e2102",
      "7ad763451f144765ab81fe9370078cc1",
      "4a63260f137d4f0b85f5f71002bdb7f4",
      "d961531ad9394a40b726b467953ff5fa",
      "4836029441514c2387b40409c375db9f",
      "c45bf67e7afb441783e1e73da3689fc0",
      "0dc6c02004d0413b8a98e1cebd9f83fe",
      "f68e61e6b4a648a692b6ae3f425a7d0b",
      "2ef120b77ec54692a438d2a998b4234c",
      "9a860f6c41f5450f8c694b83149bf22b",
      "8eeaefa2d769498ea31a63a3e39575c1"
     ]
    },
    "id": "LtdIWf5SwKYn",
    "outputId": "854ab238-7751-4a40-f561-06bb4bda21c8"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # Set this to 60 for a quick demo, or 1000 for real training\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gktPJZPwNaN",
    "outputId": "fbe37868-dc41-4f41-d74d-eae131316b3e"
   },
   "outputs": [],
   "source": [
    "# 5. Run Inference (Test it)\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"List the prime numbers between 1 and 100.\", # Instruction\n",
    "        \"\", # Input\n",
    "        \"\", # Output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGNA7YZ9wUu4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
