{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DataSavvyYT/experiments/blob/main/1_llm_finetune/1_catastropic_foregetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "430zhOo5Powm",
    "outputId": "41405dbf-b16c-4e9a-cc73-b9b313d200d1"
   },
   "outputs": [],
   "source": [
    "# Install deps (Colab)\n",
    "%pip -q install -U transformers accelerate datasets peft bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHSCVd-NqFQY"
   },
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from google.colab import drive\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFUVIeWugmsw",
    "outputId": "08d0bc14-8ab3-4c97-d659-89bf186bb351"
   },
   "outputs": [],
   "source": [
    "# 2. Mount Google Drive\n",
    "# This lets Colab see your \"local\" files in Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ====================================================\n",
    "# CHANGE THIS PATH to point to your model folder\n",
    "# Example: \"/content/drive/MyDrive/AI_Models/gemma-2b\"\n",
    "# The folder must contain 'config.json' and 'model.safetensors'\n",
    "# ====================================================\n",
    "local_model_path = \"/content/drive/MyDrive/models/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saeL162xIZ-t"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"  # or gemma-2-2b, choose -it for instruction-tuned base\n",
    "OUTPUT_DIR = \"gemma-promo-qlora\"\n",
    "MAX_SEQ_LEN = 512\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM = 4\n",
    "EPOCHS = 1\n",
    "LR = 2e-4\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "DATASET_PATH = \"json\"  # \"json\" + data_files below, or point to your HF dataset repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "717442b75dc549baa30f44d7fe879f9e",
      "84c915d6618a4e0eb252395930f6fc7a",
      "c24fa67f8fe7466da6d61a1d32251305",
      "f33a906d3b3b4328a9da4e0784acd112",
      "65f879acb70d43738157af091ff2be6c",
      "935c130b053e4c268e2f6d7f86ce556f",
      "9e3b30836ecd4b4fa68e276b737ca211",
      "11453159ef3c4ddcabb05ad2282bcf90",
      "4b77e46f48a147e9bd99f214cbefd876",
      "9e3cfd34027f4511a976b0f20045a3eb",
      "3296f1f01a53478a98080ccd181c014a",
      "4d4c85bb2fb24240abb99b862a485aac",
      "3d2fcc958f9c493cbb23e635a85d49eb",
      "5a050948ad824d518e977429ed5cb50f",
      "51b867dfe3274c7c9230c11a55c61086",
      "7bcc4781fcdb46578810e235f89793d0",
      "39effa8cb3784edd90263f04c74681ad",
      "6a6af154f4d04db592524a280b1e86ca",
      "4071705ab22f465094754ecf2f86bd3e",
      "938ad8c6eadf4673bd87836ac5f9a9c2",
      "757d8813a49c4e8ebb22f8c4071bab91",
      "3b380fef0ab644eabf1d8c16953c9296"
     ]
    },
    "id": "iAjRHgzhIgip",
    "outputId": "3a160688-3a41-4a74-da50-f96bf8c1b61c"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Load dataset\n",
    "# Expect JSONL with fields: instruction, input, output\n",
    "# ----------------------------\n",
    "dataset = load_dataset(\n",
    "    DATASET_PATH,\n",
    "    data_files={\n",
    "        \"train\": \"/content/drive/MyDrive/data/promotion/train.jsonl\",\n",
    "        \"validation\": \"/content/drive/MyDrive/data/promotion/validation.jsonl\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpY_JCRLIjN0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Formatting function to create supervised prompts\n",
    "SYSTEM_PREFIX = \"You are an analyst that predicts promotion effectiveness based on campaign details.\"\n",
    "INSTR_TEMPLATE = \"\"\"<system>\n",
    "{system}\n",
    "</system>\n",
    "<instruction>\n",
    "{instruction}\n",
    "</instruction>\n",
    "<input>\n",
    "{inp}\n",
    "</input>\n",
    "<output>\n",
    "{out}\n",
    "</output>\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8d4d649ba47248938e1983253fad3d28",
      "589615bfe6d047ce890f2b7fb2f4c1bb",
      "106b9212cc1d458e882feb9bb85738be",
      "431d10d0eb4a48d88e62bf4518ede88c",
      "90332aeb8b924f41b8c77f2a17135931",
      "29a41280a72c49d2b4fc93917df43c04",
      "0211028778314673a21ee480fb6c1f72",
      "c302f59f18ff4fbc8b346886c91e73ee",
      "38b9469c3df74577949ffb38efe726b0",
      "8d61b3633a9e4f57a09c8943901c9e2b",
      "0257c5a9d9c64c0dac7f9c66273b2ad2",
      "d7fc867346fe4c70b4e1abecc484e765",
      "ada248fbda52417490eab17e3dfd4105",
      "068e9339b5104435b1a33c767feb8465",
      "0d25642b6e2d416d9939dceba411b80b",
      "24b57aba3ab241f59393d6616bcdf847",
      "42b9cc073b394666be80eafeab6bc429",
      "515001d38a714b6aa0e61ee4bbb5fbd9",
      "8b366a16401e44db966a94e4720d8835",
      "503828994cbc41cfa424e4640af08b06",
      "2e5c8fc68f5943a59b0bd8f6c05ef8a1",
      "d380026954c84bf08d7e532adf517406"
     ]
    },
    "id": "K2VSQZ-eIq43",
    "outputId": "81024173-4401-42c0-80e1-fbc992b04f3c"
   },
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    instruction = example.get(\"instruction\", \"Predict promotion effectiveness.\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    out = example.get(\"output\", \"\")\n",
    "    # SFTTrainer learns to map input -> output; include output as labels portion\n",
    "    return INSTR_TEMPLATE.format(system=SYSTEM_PREFIX, instruction=instruction, inp=inp, out=out)\n",
    "\n",
    "def map_fn(batch):\n",
    "    texts = [format_example(ex) for ex in batch]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_ds = dataset[\"train\"].map(lambda ex: {\"text\": format_example(ex)})\n",
    "eval_ds = dataset[\"validation\"].map(lambda ex: {\"text\": format_example(ex)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wktLDYOItne"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Tokenizer & 4-bit model load (QLoRA)\n",
    "# ----------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "675b5c5a7e6944f2a2fefb09324f31c3",
      "9379ed2ade854342a4dc0bcd1e2a10e8",
      "a67d95a88d59494b891ceeeb0f4f9768",
      "0699e37c117645d4a70b800ea139adc1",
      "b0624f6a3f314da1a5c6a4ee4d56d08c",
      "5438dbf672b64f1a82fe7f10de68d8d6",
      "67395374ae514873894650d9adc28b43",
      "6d4354267d324fb5b2b550bce64ce80d",
      "981948b1d55842b4af71a86b0853dd47",
      "d3e44ee3e5ae4e3ba306df95edb5d96e",
      "1ae69d36093b42859088e91539b3d646",
      "c2a6da227c7e443493dba7168ee7aa6c",
      "9fd94bc1bbbc455c966acd7eaa13b762",
      "237030a3f59446d7954255e06585a61f",
      "28bf0daa410844be8e23d57e8f9983b4",
      "819c841d55e14bc58e93c81201cf52a8",
      "4c198e81b08c423f9ed4a4e009981be9",
      "91dbe33297a2485f96ee475f9b62a11b",
      "c21a2148e36849c59e9707f077356dc5",
      "92cf6aa81fea45008b843994c46801c9",
      "084b4b436f5845daa1a958047dfab1bb",
      "b4a923f643f04a33a52057a7254db175",
      "180a3050da5d433a9ac7016be371f7c0",
      "6d3d02e69412472992880acfe3e64d59",
      "5a56a47b29414043b19fc4de3028ef48",
      "6c97ffbabd054f43916c61e34aee91e3",
      "81e6ce2fd667491dbee4614e30b8c5c6",
      "e05b71fda3a7470eb668e2537e0e2dd4",
      "6a556624726b48eaacc0f46455483019",
      "695df14eaceb44438aa2fee695215a03",
      "052ea94af07741aea3d1ca3058f13d1c",
      "e4554cb28ed941bdaa901bd503a049a0",
      "b509c7654531456a915f5a0cdce71d6f",
      "6b1ffa83129341118827e14e056fc534",
      "37c7e08c91ca4941bd51db89c2a5e2cc",
      "00f8806e1b3b4a5782e7b21763ccf6e1",
      "8279460a61554022ba9dba7363c85864",
      "f05824a316014509bcacefa2e1b95ef7",
      "93130df0d6a24de888d9432008e26431",
      "ab728c05b3994a039929f6f2fe489c3a",
      "f36d22a1afba4acca5c2501c646087b8",
      "9c9338e195924dda910ab0559f4fd91e",
      "03edc27041154c6dbba31adfb7af35f5",
      "86cafa11a567419bad66affaf42b6d99",
      "3d08f578990b40bc85159e819ede42d9",
      "39cc660813b84ec6bab16c91b9cef313",
      "17c08be3bf1a4c6196d88fb92e6f730c",
      "0cfbb87114aa4e1ca544da2a85017f70",
      "aab1c1ebd7244835bb56d1e13697ea21",
      "ea2bf1f7cf8b471cbb4adfd695f91908",
      "d5736cca50f9416fbb6eed1444150e5f",
      "d17ef20c5b004cafa1435963541538f6",
      "4aafbe1326794a4bb0bf51fadd489cb8",
      "da004495e3e246cb91081602bdb19b28",
      "5164dd791b164352a0a69b087e99626e"
     ]
    },
    "id": "vzesx9pqIwnn",
    "outputId": "6b171d41-0d25-4c02-d5f9-8cb6f5377961"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "#    BASE_MODEL,\n",
    "    local_model_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_Elti_FIzkl"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LoRA config (PEFT)\n",
    "# ----------------------------\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYowP5zwin0J"
   },
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    # Ensure truncation is handled correctly, and pad if necessary for batching (though SFTTrainer handles padding generally)\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LEN)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy() # For causal LMs, labels are usually input_ids\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_ds = train_ds.map(tokenize_function, batched=True, remove_columns=train_ds.column_names)\n",
    "tokenized_eval_ds = eval_ds.map(tokenize_function, batched=True, remove_columns=eval_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubRXsyiGI1eF"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Trainer\n",
    "# ----------------------------\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    #per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    optim = \"adamw_8bit\",\n",
    "    bf16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "4c7b8bb6",
    "outputId": "3bd89b75-65d1-480d-f9fe-de63af6a9ff4"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdXiHH7qI49A"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: Merge LoRA into base weights (for single file deployment)\n",
    "# Note: requires reloading base model in full precision or 8-bit for merge\n",
    "# ----------------------------\n",
    "# from peft import PeftModel\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# base_fp = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# peft_model = PeftModel.from_pretrained(base_fp, OUTPUT_DIR)\n",
    "# merged = peft_model.merge_and_unload()\n",
    "# merged.save_pretrained(\"gemma-promo-merged\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "# tokenizer.save_pretrained(\"gemma-promo-merged\")\n",
    "\n",
    "# ----------------------------\n",
    "# Quick eval helper: generate prediction for a sample\n",
    "# ----------------------------\n",
    "def predict_effectiveness(description: str) -> str:\n",
    "    prompt = f\"\"\"<system>\n",
    "{SYSTEM_PREFIX}\n",
    "</system>\n",
    "<instruction>\n",
    "Predict promotion effectiveness as one of: \"effective\", \"not effective\", or a probability between 0 and 1.\n",
    "</instruction>\n",
    "<input>\n",
    "{description}\n",
    "</input>\n",
    "<output>\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # naive parse: take last line after <output>\n",
    "    return text.split(\"<output>\")[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLhnm6UBQQ-h"
   },
   "outputs": [],
   "source": [
    "print(predict_effectiveness(\"Campaign: Diwali Sale; Channel: Email; Budget: 5 Lakh INR; Audience: Returning; Discount: 10%; Duration: 5 days; Past CTR: 2.8%\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9icOGm9TCzW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOcUFN66TL2iLdjWSM07Hzf",
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "https://github.com/DataSavvyYT/experiments/blob/main/1_llm_finetune/1_catastropic_foregetting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
