{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e17249d55d354e0799c59345b54f5cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ced53c7b5d0c472998f7f589c13e4b77",
              "IPY_MODEL_9e84acf94e9a426a8a4fcc141b52ea4e",
              "IPY_MODEL_49532655f5444864a1f1b4cde8c88fda"
            ],
            "layout": "IPY_MODEL_58f6f5b625244623888e3c9a593d687d"
          }
        },
        "ced53c7b5d0c472998f7f589c13e4b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2026250ce322484ebbffae476aec5a8e",
            "placeholder": "​",
            "style": "IPY_MODEL_63b581b8e97c4065a4ee036040a04e13",
            "value": "Epoch 1/2: 100%"
          }
        },
        "9e84acf94e9a426a8a4fcc141b52ea4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e9ef002fc2407f8f256d308ee695cf",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a20bbb48da0c491ab6ac1d01bf3a5379",
            "value": 125
          }
        },
        "49532655f5444864a1f1b4cde8c88fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994ee6cf63b94a35960f0ab5aa255d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd4588685cb4450aec2301b7b98bb80",
            "value": " 125/125 [00:25&lt;00:00,  4.29it/s, loss=4.2720, avg_loss=4.1349]"
          }
        },
        "58f6f5b625244623888e3c9a593d687d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2026250ce322484ebbffae476aec5a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63b581b8e97c4065a4ee036040a04e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9e9ef002fc2407f8f256d308ee695cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a20bbb48da0c491ab6ac1d01bf3a5379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "994ee6cf63b94a35960f0ab5aa255d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd4588685cb4450aec2301b7b98bb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36af4c0325884fb1b7d072f2199264b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c39cd578528c44148231b353f04614de",
              "IPY_MODEL_fa48d4607bff4c7889027c4c11e9b8b0",
              "IPY_MODEL_9d4c6a856c5a4dba8232f4d3e2ae3769"
            ],
            "layout": "IPY_MODEL_a22e947d9c6b421a9d8f3d75cca738c6"
          }
        },
        "c39cd578528c44148231b353f04614de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_145ebcc8968a4576bb1d7d9fd45ed382",
            "placeholder": "​",
            "style": "IPY_MODEL_7f2beba1280741bb9eb865b1f454aeeb",
            "value": "Epoch 2/2: 100%"
          }
        },
        "fa48d4607bff4c7889027c4c11e9b8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab08e7f47824b5fa33c5774b19de514",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9dfc0b7e5404f8f91e7b15151ffb1eb",
            "value": 125
          }
        },
        "9d4c6a856c5a4dba8232f4d3e2ae3769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2113bfea7e54e74820c0f1c0aa94675",
            "placeholder": "​",
            "style": "IPY_MODEL_34802f3a84194304bb3acd3a85f6e442",
            "value": " 125/125 [00:25&lt;00:00,  3.64it/s, loss=3.9755, avg_loss=1.8379]"
          }
        },
        "a22e947d9c6b421a9d8f3d75cca738c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "145ebcc8968a4576bb1d7d9fd45ed382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f2beba1280741bb9eb865b1f454aeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ab08e7f47824b5fa33c5774b19de514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9dfc0b7e5404f8f91e7b15151ffb1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2113bfea7e54e74820c0f1c0aa94675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34802f3a84194304bb3acd3a85f6e442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataSavvyYT/experiments/blob/main/paper_implement/dev/1_mrinal_mtech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets sentence-transformers torch accelerate"
      ],
      "metadata": {
        "id": "15Lqhuu9EfYO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    T5ForConditionalGeneration,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "metadata": {
        "id": "EyAfWp7FEh9P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "wLnV1PPYEkum"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T78OVKbwEnpR",
        "outputId": "11e06d99-9103-4bc6-e3e8-b4f962ef1428"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Configuration ====================\n",
        "class Config:\n",
        "    # Model parameters\n",
        "    llm_name = \"google/flan-t5-base\"  # Using base instead of XXL for Colab\n",
        "    encoder_name = \"BAAI/bge-base-en-v1.5\"  # BGE encoder as per paper\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 4\n",
        "    learning_rate = 1e-4\n",
        "    num_epochs = 2\n",
        "    warmup_ratio = 0.05\n",
        "    max_input_length = 256\n",
        "    max_encoder_length = 512\n",
        "    max_output_length = 128\n",
        "\n",
        "    # PPlug specific\n",
        "    embedding_dim = 768  # BGE base embedding dimension\n",
        "    llm_hidden_size = 768  # T5-base hidden size\n",
        "    num_personal_tokens = 1  # Number of personal embedding tokens\n",
        "\n",
        "    # Data parameters\n",
        "    max_histories = 20  # Limit user histories for memory efficiency\n",
        "    sample_size = 500  # Reduced dataset size for Colab\n"
      ],
      "metadata": {
        "id": "H672ZcQYEqEZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()"
      ],
      "metadata": {
        "id": "4xW2x8MlEtrd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Data Loading ====================\n",
        "class LaMP_Dataset:\n",
        "    \"\"\"Simplified LaMP dataset loader for demonstration\"\"\"\n",
        "\n",
        "    def __init__(self, task=\"LaMP-2\", split=\"train\", sample_size=500):\n",
        "        \"\"\"\n",
        "        Load LaMP dataset (using LaMP-2: Movie Tagging as example)\n",
        "        For full implementation, download from: https://lamp-benchmark.github.io/download\n",
        "        \"\"\"\n",
        "        print(f\"Loading {task} {split} dataset...\")\n",
        "\n",
        "        # For demo: Create synthetic data mimicking LaMP structure\n",
        "        # In production, load from: https://huggingface.co/datasets/LaMP/LaMP-2\n",
        "        self.data = self._create_demo_data(sample_size)\n",
        "\n",
        "    def _create_demo_data(self, sample_size):\n",
        "        \"\"\"Create CONSISTENT synthetic data for demonstration\"\"\"\n",
        "        data = []\n",
        "\n",
        "        # Simulate movie tagging task\n",
        "        movie_genres = ['Action', 'Comedy', 'Drama', 'Horror', 'Sci-Fi',\n",
        "                       'Romance', 'Thriller', 'Documentary']\n",
        "\n",
        "        for i in range(sample_size):\n",
        "            user_id = f\"user_{i % 50}\"\n",
        "\n",
        "            # 1. Select a specific genre for this sample\n",
        "            selected_genre = np.random.choice(movie_genres)\n",
        "\n",
        "            # 2. Create input text that actually contains hints for this genre\n",
        "            current_movie = f\"Movie: Test Film {i}. Description: An exciting {selected_genre.lower()} adventure with classic tropes.\"\n",
        "\n",
        "            # 3. Set the output to match the selected genre\n",
        "            target_genre = selected_genre\n",
        "\n",
        "            # 4. Create consistent history (user tends to watch this genre)\n",
        "            histories = []\n",
        "            num_hist = np.random.randint(2, 5) # Smaller history for faster processing\n",
        "            for j in range(num_hist):\n",
        "                # User history also aligns with their preference often, but includes noise\n",
        "                if np.random.random() > 0.3:\n",
        "                    hist_genre = selected_genre\n",
        "                else:\n",
        "                    hist_genre = np.random.choice(movie_genres)\n",
        "\n",
        "                hist = {\n",
        "                    'text': f\"Movie: Old Film {j}. Description: A typical {hist_genre.lower()} movie.\",\n",
        "                    'label': hist_genre\n",
        "                }\n",
        "                histories.append(hist)\n",
        "\n",
        "            data.append({\n",
        "                'user_id': user_id,\n",
        "                'input': current_movie,\n",
        "                'output': target_genre,\n",
        "                'histories': histories\n",
        "            })\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "QcWPKR9_E4vg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== User Behavior Encoder ====================\n",
        "class UserBehaviorEncoder(nn.Module):\n",
        "    \"\"\"Encodes user historical behaviors using BGE model\"\"\"\n",
        "\n",
        "    def __init__(self, encoder_name):\n",
        "        super().__init__()\n",
        "        self.encoder = SentenceTransformer(encoder_name)\n",
        "        # Freeze encoder parameters as per paper\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def encode_histories(self, histories):\n",
        "        \"\"\"Encode list of historical behaviors\"\"\"\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.encoder.encode(\n",
        "                histories,\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        return embeddings\n",
        "\n",
        "    def encode_input(self, inputs, trainable=True):\n",
        "        \"\"\"Encode current input (can be trainable)\"\"\"\n",
        "        if trainable:\n",
        "            embeddings = self.encoder.encode(\n",
        "                inputs,\n",
        "                convert_to_tensor=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                embeddings = self.encoder.encode(\n",
        "                    inputs,\n",
        "                    convert_to_tensor=True,\n",
        "                    show_progress_bar=False\n",
        "                )\n",
        "        return embeddings\n"
      ],
      "metadata": {
        "id": "moAaHzueE9o-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    T5ForConditionalGeneration,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# ==================== Input-aware Personal Aggregator ====================\n",
        "class PersonalAggregator(nn.Module):\n",
        "    \"\"\"Aggregates user histories into personal embedding with attention\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, llm_hidden_size):\n",
        "        super().__init__()\n",
        "        # Project from encoder space to LLM space\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, llm_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(llm_hidden_size, llm_hidden_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, history_embeddings, input_embedding):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            history_embeddings: [num_histories, embedding_dim]\n",
        "            input_embedding: [embedding_dim]\n",
        "        Returns:\n",
        "            personal_embedding: [llm_hidden_size]\n",
        "        \"\"\"\n",
        "        # Ensure history_embeddings is not an inference tensor and does not require gradients\n",
        "        history_embeddings_processed = history_embeddings.clone().detach() # FIX: Added .clone()\n",
        "\n",
        "        # Compute attention weights (Equation 3 in paper)\n",
        "        # wi = exp(xu^T * hu_i) / sum(exp(xu^T * hu_k))\n",
        "        scores = torch.matmul(history_embeddings_processed, input_embedding)  # [num_histories]\n",
        "        weights = torch.softmax(scores, dim=0)  # [num_histories]\n",
        "\n",
        "        # Weighted aggregation (Equation 4 in paper)\n",
        "        # Pu = sum(wi * Proj(hu_i))\n",
        "        projected_histories = self.projector(history_embeddings_processed)  # [num_histories, llm_hidden_size]\n",
        "        personal_embedding = torch.sum(\n",
        "            weights.unsqueeze(1) * projected_histories,\n",
        "            dim=0\n",
        "        )  # [llm_hidden_size]\n",
        "\n",
        "        return personal_embedding, weights\n"
      ],
      "metadata": {
        "id": "0P_Kz3LuFEai"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    T5ForConditionalGeneration,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# ==================== PPlug Model ====================\n",
        "class PPlugModel(nn.Module):\n",
        "    \"\"\"Corrected PPlug model with proper initialization and masking\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load LLM (frozen)\n",
        "        self.llm = T5ForConditionalGeneration.from_pretrained(config.llm_name)\n",
        "        self.llm_tokenizer = AutoTokenizer.from_pretrained(config.llm_name)\n",
        "\n",
        "        # Freeze LLM parameters\n",
        "        for param in self.llm.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # User behavior encoder\n",
        "        self.behavior_encoder = UserBehaviorEncoder(config.encoder_name)\n",
        "\n",
        "        # Personal aggregator (trainable)\n",
        "        self.personal_aggregator = PersonalAggregator(\n",
        "            config.embedding_dim,\n",
        "            config.llm_hidden_size\n",
        "        )\n",
        "\n",
        "        # --- FIX 1: Smaller Initialization ---\n",
        "        # Initialize with 0.01 scale to match T5's internal embedding variance\n",
        "        self.instruction_embedding = nn.Parameter(\n",
        "            torch.randn(1, config.num_personal_tokens, config.llm_hidden_size) * 0.01\n",
        "        )\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def get_personal_embedding(self, histories, current_input):\n",
        "        \"\"\"Generate personal embedding for a user\"\"\"\n",
        "        # Encode histories (limit to max_histories to save memory)\n",
        "        history_texts = [h['text'] for h in histories[:self.config.max_histories]]\n",
        "\n",
        "        # Handle case where history might be empty (though unlikely with our data generator)\n",
        "        if not history_texts:\n",
        "            history_texts = [\"Empty history\"]\n",
        "\n",
        "        history_embeddings = self.behavior_encoder.encode_histories(history_texts)\n",
        "\n",
        "        # Encode current input\n",
        "        input_embedding = self.behavior_encoder.encode_input([current_input], trainable=True)[0]\n",
        "\n",
        "        # Aggregate into personal embedding\n",
        "        personal_embedding, attention_weights = self.personal_aggregator(\n",
        "            history_embeddings.to(self.instruction_embedding.device),\n",
        "            input_embedding.to(self.instruction_embedding.device)\n",
        "        )\n",
        "\n",
        "        return personal_embedding, attention_weights\n",
        "\n",
        "    def forward(self, batch):\n",
        "        batch_size = len(batch['input'])\n",
        "        device = self.instruction_embedding.device\n",
        "\n",
        "        # Tokenize input (we need the mask this time!)\n",
        "        tokenized_inputs = self.llm_tokenizer(\n",
        "            batch['input'],\n",
        "            max_length=self.config.max_input_length,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = tokenized_inputs.input_ids\n",
        "        original_mask = tokenized_inputs.attention_mask\n",
        "\n",
        "        # Tokenize labels\n",
        "        labels = self.llm_tokenizer(\n",
        "            batch['output'],\n",
        "            max_length=self.config.max_output_length,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).input_ids.to(device)\n",
        "        labels[labels == self.llm_tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Get original input embeddings\n",
        "        inputs_embeds = self.llm.encoder.embed_tokens(input_ids)\n",
        "\n",
        "        # Create personal embeddings\n",
        "        personal_embeds_list = []\n",
        "        for i in range(batch_size):\n",
        "            personal_emb, _ = self.get_personal_embedding(\n",
        "                batch['histories'][i],\n",
        "                batch['input'][i]\n",
        "            )\n",
        "            personal_embeds_list.append(personal_emb)\n",
        "\n",
        "        personal_embeds = torch.stack(personal_embeds_list).unsqueeze(1) # [batch, 1, hidden]\n",
        "\n",
        "        # Concatenate embeddings: [Instruction; Personal; Input]\n",
        "        instruction_embeds = self.instruction_embedding.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Total added tokens = num_personal_tokens (instruction) + 1 (personal embedding)\n",
        "        num_prefix_tokens = self.config.num_personal_tokens + 1\n",
        "\n",
        "        final_embeds = torch.cat([\n",
        "            instruction_embeds,\n",
        "            personal_embeds,\n",
        "            inputs_embeds\n",
        "        ], dim=1)\n",
        "\n",
        "        # --- FIX 2: Correct Attention Mask ---\n",
        "        # Create mask of 1s for the prefix embeddings\n",
        "        prefix_mask = torch.ones(batch_size, num_prefix_tokens).to(device)\n",
        "        # Concatenate with original mask\n",
        "        final_mask = torch.cat([prefix_mask, original_mask], dim=1)\n",
        "\n",
        "        # Forward through LLM\n",
        "        outputs = self.llm(\n",
        "            inputs_embeds=final_embeds,\n",
        "            attention_mask=final_mask,  # Pass the corrected mask\n",
        "            labels=labels,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        return outputs.loss, outputs.logits\n",
        "\n",
        "    def generate(self, input_text, histories, max_length=50):\n",
        "        device = self.instruction_embedding.device\n",
        "\n",
        "        # Get personal embedding\n",
        "        personal_emb, attention_weights = self.get_personal_embedding(histories, input_text)\n",
        "\n",
        "        # Tokenize input to get IDs and Mask\n",
        "        tokenized = self.llm_tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.config.max_input_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = tokenized.input_ids\n",
        "        original_mask = tokenized.attention_mask\n",
        "        inputs_embeds = self.llm.encoder.embed_tokens(input_ids)\n",
        "\n",
        "        # Prepare embeddings\n",
        "        personal_embeds = personal_emb.unsqueeze(0).unsqueeze(0)\n",
        "        final_embeds = torch.cat([\n",
        "            self.instruction_embedding,\n",
        "            personal_embeds,\n",
        "            inputs_embeds\n",
        "        ], dim=1)\n",
        "\n",
        "        # Prepare Mask for Generation\n",
        "        num_prefix_tokens = self.config.num_personal_tokens + 1\n",
        "        prefix_mask = torch.ones(1, num_prefix_tokens).to(device)\n",
        "        final_mask = torch.cat([prefix_mask, original_mask], dim=1)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.llm.generate(\n",
        "                inputs_embeds=final_embeds,\n",
        "                attention_mask=final_mask, # Pass mask here too\n",
        "                max_length=max_length,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        output_text = self.llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return output_text, attention_weights"
      ],
      "metadata": {
        "id": "iiA_QidzFISV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Training ====================\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
        "    return {\n",
        "        'input': [item['input'] for item in batch],\n",
        "        #'output': [item['output'] for item in batch],\n",
        "        'histories': [item['histories'] for item in batch],\n",
        "        'user_id': [item['user_id'] for item in batch],\n",
        "        'output': [item['output'] for item in batch]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "tGtr1W6CFRdy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pplug(model, train_dataset, config):\n",
        "    \"\"\"Train PPlug model\"\"\"\n",
        "\n",
        "    # Create dataloader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # Optimizer (only trainable parameters)\n",
        "    optimizer = AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=config.learning_rate\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    num_training_steps = len(train_loader) * config.num_epochs\n",
        "    num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "       # Training loop\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Forward pass\n",
        "            loss, logits = model(batch)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update metrics\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg_loss': f'{epoch_loss/global_step:.4f}'\n",
        "            })\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} completed. Average loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "HbK6ABNLFTjl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Evaluation ====================\n",
        "def evaluate_pplug(model, test_dataset, num_samples=10):\n",
        "    \"\"\"Evaluate PPlug model on test set with robust text matching\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EVALUATION EXAMPLES\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i in range(min(num_samples, len(test_dataset))):\n",
        "        sample = test_dataset[i]\n",
        "\n",
        "        # Generate prediction\n",
        "        pred_text, attention_weights = model.generate(\n",
        "            sample['input'],\n",
        "            sample['histories'],\n",
        "            max_length=config.max_output_length\n",
        "        )\n",
        "\n",
        "        # CLEANUP: Normalize text for comparison\n",
        "        pred_clean = pred_text.strip().lower()\n",
        "        truth_clean = sample['output'].strip().lower()\n",
        "\n",
        "        predictions.append(pred_clean)\n",
        "        ground_truths.append(truth_clean)\n",
        "\n",
        "        # Print examples\n",
        "        if i < 5:\n",
        "            print(f\"\\n--- Example {i+1} ---\")\n",
        "            print(f\"Input: {sample['input'][:80]}...\")\n",
        "            print(f\"Predicted: '{pred_text}' (Cleaned: '{pred_clean}')\")\n",
        "            print(f\"Ground Truth: '{sample['output']}' (Cleaned: '{truth_clean}')\")\n",
        "            print(f\"Match: {pred_clean == truth_clean}\")\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(ground_truths, predictions)\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    return predictions, ground_truths, accuracy\n"
      ],
      "metadata": {
        "id": "IvR2WDUVFXpX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Main Execution ====================\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"PPlug: Personalized LLM Implementation\")\n",
        "    print(\"Based on: LLMs + Persona-Plug = Personalized LLMs\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n1. Loading datasets...\")\n",
        "    train_dataset = LaMP_Dataset(task=\"LaMP-2\", split=\"train\", sample_size=config.sample_size)\n",
        "    test_dataset = LaMP_Dataset(task=\"LaMP-2\", split=\"test\", sample_size=100)\n",
        "    print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\n2. Initializing PPlug model...\")\n",
        "    model = PPlugModel(config).to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n3. Training PPlug model...\")\n",
        "    model = train_pplug(model, train_dataset, config)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\n4. Evaluating PPlug model...\")\n",
        "    predictions, ground_truths, accuracy = evaluate_pplug(model, test_dataset)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training and Evaluation Complete!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return model, predictions, ground_truths\n"
      ],
      "metadata": {
        "id": "fi2LbklzFjNG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, predictions, ground_truths = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e17249d55d354e0799c59345b54f5cff",
            "ced53c7b5d0c472998f7f589c13e4b77",
            "9e84acf94e9a426a8a4fcc141b52ea4e",
            "49532655f5444864a1f1b4cde8c88fda",
            "58f6f5b625244623888e3c9a593d687d",
            "2026250ce322484ebbffae476aec5a8e",
            "63b581b8e97c4065a4ee036040a04e13",
            "d9e9ef002fc2407f8f256d308ee695cf",
            "a20bbb48da0c491ab6ac1d01bf3a5379",
            "994ee6cf63b94a35960f0ab5aa255d9a",
            "1bd4588685cb4450aec2301b7b98bb80",
            "36af4c0325884fb1b7d072f2199264b0",
            "c39cd578528c44148231b353f04614de",
            "fa48d4607bff4c7889027c4c11e9b8b0",
            "9d4c6a856c5a4dba8232f4d3e2ae3769",
            "a22e947d9c6b421a9d8f3d75cca738c6",
            "145ebcc8968a4576bb1d7d9fd45ed382",
            "7f2beba1280741bb9eb865b1f454aeeb",
            "6ab08e7f47824b5fa33c5774b19de514",
            "b9dfc0b7e5404f8f91e7b15151ffb1eb",
            "b2113bfea7e54e74820c0f1c0aa94675",
            "34802f3a84194304bb3acd3a85f6e442"
          ]
        },
        "id": "_HGbpj5pFnrH",
        "outputId": "2e849f4c-ee47-4336-ed86-ca066f063840"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PPlug: Personalized LLM Implementation\n",
            "Based on: LLMs + Persona-Plug = Personalized LLMs\n",
            "============================================================\n",
            "\n",
            "1. Loading datasets...\n",
            "Loading LaMP-2 train dataset...\n",
            "Loading LaMP-2 test dataset...\n",
            "Train size: 500, Test size: 100\n",
            "\n",
            "2. Initializing PPlug model...\n",
            "Total parameters: 358,242,048\n",
            "Trainable parameters: 1,181,952 (0.33%)\n",
            "\n",
            "3. Training PPlug model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/2:   0%|          | 0/125 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e17249d55d354e0799c59345b54f5cff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 completed. Average loss: 4.1349\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/2:   0%|          | 0/125 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36af4c0325884fb1b7d072f2199264b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 completed. Average loss: 3.6757\n",
            "\n",
            "4. Evaluating PPlug model...\n",
            "\n",
            "==================================================\n",
            "EVALUATION EXAMPLES\n",
            "==================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "Input: Movie: Test Film 0. Description: An exciting romance adventure with classic trop...\n",
            "Predicted: 'PG-13' (Cleaned: 'pg-13')\n",
            "Ground Truth: 'Romance' (Cleaned: 'romance')\n",
            "Match: False\n",
            "\n",
            "--- Example 2 ---\n",
            "Input: Movie: Test Film 1. Description: An exciting drama adventure with classic tropes...\n",
            "Predicted: 'Thriller' (Cleaned: 'thriller')\n",
            "Ground Truth: 'Drama' (Cleaned: 'drama')\n",
            "Match: False\n",
            "\n",
            "--- Example 3 ---\n",
            "Input: Movie: Test Film 2. Description: An exciting thriller adventure with classic tro...\n",
            "Predicted: 'Thriller' (Cleaned: 'thriller')\n",
            "Ground Truth: 'Thriller' (Cleaned: 'thriller')\n",
            "Match: True\n",
            "\n",
            "--- Example 4 ---\n",
            "Input: Movie: Test Film 3. Description: An exciting action adventure with classic trope...\n",
            "Predicted: 'Thriller' (Cleaned: 'thriller')\n",
            "Ground Truth: 'Action' (Cleaned: 'action')\n",
            "Match: False\n",
            "\n",
            "--- Example 5 ---\n",
            "Input: Movie: Test Film 4. Description: An exciting drama adventure with classic tropes...\n",
            "Predicted: 'PG-13' (Cleaned: 'pg-13')\n",
            "Ground Truth: 'Drama' (Cleaned: 'drama')\n",
            "Match: False\n",
            "\n",
            "==================================================\n",
            "Accuracy: 0.1000\n",
            "==================================================\n",
            "\n",
            "============================================================\n",
            "Training and Evaluation Complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ3ic4KGFpv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}