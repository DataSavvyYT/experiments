{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EH1xhzXN4ut"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0c7AvdLN4ut"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464,
     "referenced_widgets": [
      "f6696ee71a84416dacd67143683a6b93",
      "40e702604e1a430ba732a7d847990848",
      "a0ccab5c2dbe4e72bf771dc9388ab43f",
      "cec032293afd447eb1a8b9eae03a0c46",
      "c34d0f8538c14548be3b37faf166132d",
      "f52ff583fd254049a1776da2d9c9017e",
      "b13736af660b4261a0232a7091855543",
      "ba8fd97a00044f14809a4fe46c41c5cc",
      "6c435846b9b5465294aa5a15f36df9b4",
      "818fce4e5d6243c4b71ca4ed1308be51",
      "c9e63bfd85be4c24943b35752b43f0a7",
      "b2ca64e3b377444f8ca5576101dd5609",
      "41081bb786ed42fa81ecf27fadf37d2d",
      "826963ac092d47ca86704f820c385043",
      "71c3db71413c4ad1b68f364171dea6c2",
      "394a127798d941b3965987cb122768fb",
      "73ee698b8aa84b7f8c3fa00a6f4c873b",
      "b8a2f5ace8424933b5f252545bed3462",
      "be47826eca7d4e6a829ebdead84f6ff1",
      "abb8a3d6d78c4654ad0a0ea591229058",
      "afd3b5cf67144d03977f83e1797fd453",
      "fe4c98b3440045288e7e43c49cc2ee5d",
      "d08ad963c3294008af05fee2b82340ba",
      "60a11e2f90824b0997b7c9d3d97b705e",
      "da26e59321344c8dbb44b4956b373f64",
      "1a498cca284348eb8e607a1f21dfe161",
      "138e412daed64c36a0faa4c21158d928",
      "722f61e7639444c5914b8cc3f68facac",
      "4b3ae214697145d9b87eeed450e9a931",
      "73e2b8fc2a0844e593bb783a2d24230d",
      "b80305c2d901496a9bacf1706de01ec0",
      "6d1616403b754688a44bb738c547d1a9",
      "942fbf254801475baa240d6b4c9feb19",
      "889a9becbc3d4feba022b12f7c239390",
      "b1fda6ed5d764d14a4d9e39479e3c4a3",
      "e9e165655d1a487b8f2204e90a0598ff",
      "ca6c1cfe52584d53b3c500353c1e93c0",
      "d5dafda4aa7d4bd5bb5ea68e5742461c",
      "ec7b68b1e6094454857a80a4a0eabbcc",
      "01f227994996479d8b0531281ce0ea66",
      "ba78318141874a78bcb57b2450553752",
      "d755d35d08a34944a4a01f32081d0d6a",
      "b8fb61fc7e984e84bed7b2b101002834",
      "1e1a093b619342d78335d2c1c6d9f550",
      "41888179a64e4bd1bb5e466835a12396",
      "e76fd9f59472464380c84ae234d17c45",
      "316a2d54cdb74db3ae319ae0348b1145",
      "b7d4da14797a4f28bfe4e0cc1277b156",
      "0e93215bb38e48cd9bf6e1009d0a0fc5",
      "3f1e57a3ab4942789c7a3f291f90c781",
      "c2a5d8b6c38844978638e3ae23949e33",
      "281b22973d50462eb30c4771671ee1df",
      "a1f9779dd6fc44659b869c449b8e78cc",
      "e8768e33ceea4783a3f00f64da14b80f",
      "9a1a7123486848d59be15742d05f7b95",
      "8e5a82629a444cbd8987c93643b213c1",
      "3c76a69b9b024450918ca22e97e55c5c",
      "77d15416f07a4a87a42abee2744db000",
      "994bbcc859614226947d12d47fbe83da",
      "b635374f6bb84a7290fe73f62d1f6561",
      "2c2bf18e603145cd9e79546b43187c70",
      "f60f0382886b4d3ab82c018b7e8dd487",
      "e790c059cb544aa6af4e65b9b3ee5d77",
      "587d205ee4f94816b1ba9c90e3decdcc",
      "3a38099dca3a4b3bbf65c1315d166dad",
      "64a334bebe2a41ceabf48df96e4b5cd9",
      "7f671591258e403c9eb1d12587f9c0a8",
      "4b2dee57de4d4b809938cb0d0d2424a1",
      "ecd5bdbbd985497abc9dfe187e3a8a44",
      "c12f5f56fca74dbc921322860859a0bd",
      "78e97343afba4980bf88fc9f0a55efb0",
      "c0eb49e9cfc3418fb6f09b4d536f8819",
      "7d12887db850448bab04f62d61e3ecb2",
      "227f516d02ee4a17abec210cd5e8a5ee",
      "be74aaaa2876413288fd1c2e1042a34b",
      "8b2c3af5f87d418daf4a28d1ddb535f9",
      "df0b665fcfc94e8584cb7e8aead23b38",
      "63ace1b5296f48ce96e087c040cbca7f",
      "40a2144868e04fd0b87edb90d83ac205",
      "4ed74074b1c649fdaaeea6ce70a7f349",
      "50ca83ed31fe432bb277298d4027a009",
      "7a2ee911a1c44f77ab0b12de162707aa",
      "065ba33776604a3db7077bef3196be43",
      "b74151c325d94b46853550b8535cb94e",
      "6033c274200f4ce3958f0da6daeb6e1b",
      "40dc25f26a394689aec8391e5b2e8e05",
      "d1089223c5b64c8895fb5ddd1b340b78",
      "6b66857da7fb4d458dbe8e6607f53aae"
     ]
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "9e60e9ba-47b9-47ba-f98d-daf47376ba79"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-270m-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "8a2927bc-d270-4fe5-ad61-98ace45ebdd7"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Gemma-3` format for conversation style finetunes. We use [Thytu's ChessInstruct](https://huggingface.co/datasets/Thytu/ChessInstruct) dataset. Gemma-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Hello!<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hey there!<end_of_turn>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTXnFWbg_OIZ"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "abbbf6b02b004288b3e1493dee2d9e54",
      "ba12ea43d27f4119bc231150a1b92b87",
      "1d119390be264e0986e9c358fa8c91be",
      "5b2bfb7c6d394b25b669effe096b3909",
      "f13d8f26f63c4e9bb753f0d4ce4b746c",
      "c22a25a232b74c7d86730bffb635f790",
      "9d5d600ab2364fbf8c3413fd13d21cb9",
      "79319062679f4f39a9bea17e9b1c87c8",
      "6f759eda8e0145cd9ce93d31c1296e02",
      "c46cc6458ac14623850266e354023255",
      "d7267447303e4f3a8542b8fd0d3e631f",
      "77db0e354fc64fc089ab363a0b6ff70c",
      "a4e53c37e71d459fba9c5ea016a52a1c",
      "517317fee5144a8984b82135e7616297",
      "9948cd5d22c148c898494fb8dcf6ab42",
      "d327dbf00d1c4cc1ace928e9ad466b3c",
      "a4e50834df304e6882272d07df983622",
      "152feb58e4fc4bdb868438b662e3e33b",
      "71037f200c494f5abe9cc6689d321876",
      "46e0a35273184d97b3a9c5dd8735579c",
      "2ed1b0c04a114b448bb09443dff39a74",
      "a838464fb7eb4c6eb33a41984d57a1f1",
      "2bbf71f27bcf474d902cf17fc96de451",
      "a1b889ba840846bebf6da049c238ac85",
      "e619456f5001437895aad19143a2158a",
      "c50ef7cb3a1d42e2aadabb11392f114a",
      "7c5f78a87e8e4386a4e12987ff5742a1",
      "8595c9c658c34aca9737b01385aaf0ec",
      "6fcd047613d540429937bdf0e7d5a37a",
      "18c7e7bb901949fc83431cc0f7f4be7d",
      "d356c2c02716483aa705d9ea84e8671f",
      "440b1b41e8684d37b35b66f034d738d2",
      "1e5a22c9f9c84fcb87ddbdb4ef00e54c",
      "9dc0feb4e895405a89183e178e8fd624",
      "44860b9b42a34135abdc3518cc3af88e",
      "6800d05356de4b4cac161dab11741844",
      "18664a0e3de84e8db2803dc295f5c296",
      "1e10668531204a7e8462e3059818fe92",
      "f3c2f797d2a9496ca0f3c9a311654af6",
      "e1f303abeb1d4d3681af7da98bf0f3a7",
      "0b9c636cc1814708a6cd6aa5bcae8d8f",
      "9470ac6a553b404197b63db97dc30784",
      "d0d5185501ce48baaa014377a4fc8100",
      "07686ceb57f64b818393737bea5fb7fa",
      "e9192927c1364f488c88fe07ed7087d3",
      "5d1312bfc2414a459adbed9d2392e930",
      "0b9f4488982c471989258644374d691b",
      "43a9663cddfd44538f46dcb6c8d32182",
      "fa2ea4adb51e432d9f093adbf1643a0a",
      "d9e9e002883e403ea24db61b6b8aacaf",
      "5efc2d6aa0924035ac9d168563dc22b8",
      "6b73f2fc7e4044f5bf17e134e7e1c390",
      "395dd4e53afc4c8582adce6cbc631efd",
      "a8d58cff866546df998747da6db69a06",
      "e6df52544418430382edcf183294ca24"
     ]
    },
    "id": "Mkq4RvEq7FQr",
    "outputId": "210f55aa-9366-4036-9b7f-e8962651e420"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Thytu/ChessInstruct\", split = \"train[:10000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "We now use `convert_to_chatml` to try converting datasets to the correct format for finetuning purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4de8a935b1a7428ca199f9cd62d53de6",
      "dd63f93aa20e424693d5312d0809c778",
      "06d53bbbe9014fe088a94e332c543d0c",
      "beeafbbc918f4cdcafbfb1d67bccba8a",
      "83490c4749164e249cd01df324c1f494",
      "25553bbb43484be0999634a1e0e18476",
      "a533946196404c53a75e5662492cd104",
      "c0a648a839e34774b7add81715de9f2b",
      "b2c09a3ae1c74c9d934a4581348fd78e",
      "6dced98ee16c4377b0f8207400e79581",
      "fe3c3f4ac70b402f99a47179ded88142"
     ]
    },
    "id": "reoBXmAn7HlN",
    "outputId": "205faf31-e275-4399-aa30-aef3a85ca055"
   },
   "outputs": [],
   "source": [
    "def convert_to_chatml(example):\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"system\", \"content\": example[\"task\"]},\n",
    "            {\"role\": \"user\", \"content\": example[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"expected_output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    convert_to_chatml\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i5Sx9In7vHi"
   },
   "source": [
    "Let's see how row 100 looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzE1OEXi7s3P",
    "outputId": "a35d2015-411e-4614-e638-4f2a689a0dad"
   },
   "outputs": [],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xs0LXio7rfd"
   },
   "source": [
    "We now have to apply the chat template for `Gemma3` onto the conversations, and save it to `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c04cb0ba403a45b7b86a9fc14b27453c",
      "aba83f9dcc7a468f9dc3cfd9572e5fbf",
      "f51ef90e456646b1be4437da9f1b8bd4",
      "75e98179b74542c591d1da95576d5c20",
      "54342aa35ac346e08a72c8b98f208a72",
      "079d7188aa2d40b1b2b456b03207f45f",
      "b4d6bbe3e28e45f78a12d3b6894dbc8f",
      "730eaae092944b1ba124a32fd703e807",
      "63d104d065814fd6bbd89f78ef0c3344",
      "192c0442ba66453288068f79a0a3ffde",
      "407e4dfd31db4b9bbd289bc10eb7f257"
     ]
    },
    "id": "1ahE8Ys37JDJ",
    "outputId": "9f859d76-5b3d-4dc1-b403-30e6b6e9f253"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "Let's see how the chat template did!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "AK86LIyn_OIa",
    "outputId": "5ecd561a-2334-4394-cdbf-8d456307a22b"
   },
   "outputs": [],
   "source": [
    "dataset[100]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 100 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "b7c41b7aa31a4e708fa53f68dced4fb5",
      "30f29d3f20a54ff7bbc8496ebfe1a37b",
      "cb51bdd21b5a40d3a390ceb7566d45f1",
      "3145b6f9635c426e98f95abf44fcc6fa",
      "61caab28453f4e7d85b524a0582c409a",
      "d2acfb83aa2f40c89d73ee88e69b2162",
      "edf0f5bb17b4479bb3942f4cf8cc8282",
      "c98d041df24b468cbe0f62c58d94ad79",
      "5b2e6a37ba9d4905aac9ab44a716b4a9",
      "2e241575d112446ab16d2dde36c00158",
      "0ef0e42169924d84a7f8de81ea486bd9"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "1309668b-0ceb-4903-f596-493f8b5f7d29"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 100,\n",
    "        learning_rate = 5e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f2b697e1dba64ca4a52095dd77000064",
      "84f6b635cf63477f85d13afe1326b601",
      "9cd00cc2707d412697b63d551bd52cd0",
      "5a89118438b54b90added7e0cb5f1051",
      "697f92293bac4cbf99f8e49f355133e0",
      "1052bfca752e44b08f8cc16a8a01187b",
      "21c03714babf4dc097ba239c5cab7c2f",
      "cd96387860984fd8afa33261ab3997d2",
      "901b398dd2e24011a0c7143cc08c2c72",
      "a1b86326bf7e4536bf009e35bff2c293",
      "73db3d90d41b4f9493faddd435e96a47"
     ]
    },
    "id": "juQiExuBG5Bt",
    "outputId": "1c4336ae-d377-407e-fdff-18b9fb262009"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "LtsMVtlkUhja",
    "outputId": "3cf8dedd-1bf5-49d4-db7e-6dc71337d4e5"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kyjy__m9KY3"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "2d72c5ca-cf65-4104-c270-e30b4c25a011"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "fb9ecc27-3f64-4829-f71e-9f370a7f421c"
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "585111a3-7d78-4c3f-86e6-bd049b9d8cce"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "d799d0f7-dc8d-4f04-a917-b0813154589a"
   },
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "e5a8c637-f23b-443c-98c3-bab44622defb"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system','content':dataset['conversations'][10][0]['content']},\n",
    "    {\"role\" : 'user', 'content' : dataset['conversations'][10][1]['content']}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 125,\n",
    "    temperature = 1, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "24de90d3-4b96-42b9-9d99-754df07b2db2"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")\n",
    "# model.push_to_hub(\"your_name/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/gemma-3\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"gemma-3\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer, save_method = \"merged_16bit\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gemma-3-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gemma-3-finetune\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"gemma-3-finetune\")\n",
    "    tokenizer.save_pretrained(\"gemma-3-finetune\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf/gemma-3-finetune\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/gemma-3-finetune\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAlzN8HKu5Ll"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3-finetune\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q974YEVPI7JS"
   },
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-TqCnZVu5Ll"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        token = \"hf_...\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
