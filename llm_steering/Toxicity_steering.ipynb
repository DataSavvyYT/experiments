{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpO9nMBo4SYK"
   },
   "source": [
    "Aim of this notebook: use activation steering to reduce toxicity of models, whilst still preserving coherent output.\n",
    "\n",
    "Constituent parts:\n",
    "1. Load sentiment analysis model (done)\n",
    "2. Have a graph to look at toxicity over tokens. (done)\n",
    "2. Create toxic or pleasant dataset\n",
    "4. Do activation steering with it (preferably for GPT-2 XL, could start trying with GPT-2 Small).\n",
    "6. Maybe think of a way to compare it to the original method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMNglYCQ5lS5"
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHCXd1dw5onl"
   },
   "source": [
    "Activation additions stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQEbOEvhDHoS"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformer_lens # Install transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "RP_5GmyVPFeA",
    "outputId": "f6353c84-8d05-4497-8df3-f3cd4fc2a669"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch\n",
    "from typing import List, Union, Tuple\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "from transformer_lens.HookedTransformer import HookedTransformer\n",
    "\n",
    "from algebraic_value_editing.completion_utils import print_n_comparisons, gen_using_activation_additions, gen_using_model\n",
    "from algebraic_value_editing.prompt_utils import get_x_vector\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams.update({\n",
    "    'font.family':'serif',\n",
    "    \"text.usetex\": False,\n",
    "    'savefig.facecolor': 'white',\n",
    "})\n",
    "\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "# plt.rc('xtick', labelsize=16)\n",
    "# plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=11)\n",
    "# plt.rc('figure', titlesize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk-aKp3EPp_6"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTyNdTt3Eye3"
   },
   "outputs": [],
   "source": [
    "# model_name = \"gpt2-xl\"\n",
    "# # model_name = \"gpt-j-6B\"\n",
    "# # model_name = \"pythia-2.8b-deduped\"\n",
    "\n",
    "# # GPT-J-6B can't load onto GPU RAM of Colab\n",
    "# device: str = \"cuda\" if (torch.cuda.is_available() and model_name != \"gpt-j-6B\") else \"cpu\"\n",
    "# model: HookedTransformer = HookedTransformer.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "1f80c7c9db994f6a8aa995c00003db2d",
      "fd0cd279118a437f9039fd11874b23d9",
      "6309327f6e334264949370853e08257d",
      "f393a7c1efba4222945312ce95c1f80f",
      "19859685d5c34904ba59c26af333c0ef",
      "51b0a91a93a64e62a04ec6a1a9bc88d1",
      "53ed1d2dd052485da7dbdb63b33a9260",
      "38886f016ba4424e9352cdc33a7d5949",
      "080a387e616e46feacdb30eee458048e",
      "5899e29d0ea74812a5f79d40771319ce",
      "6fb50d7d3d734aba816b6236b730938c",
      "c41e07afb7a14a88bc7826a5b3cbed1c",
      "601e55db37d64ff48d978d72de7ab329",
      "4d330362f85b446083c70ac2efaefe4a",
      "156e63730d564a359a9801328652e17b",
      "ce72e37022ba49d3b5899ab6a3c85209",
      "2d8ddbf7a2a1451190fa044c0b728bc9",
      "dcd763d7d32e49329fa52ef29bef1988",
      "bff1be3fe4bc432a81cbf4ad0b268c31",
      "978d2399a4e44b7e8275859c56fc3f50",
      "4b2bf130fef74e23a3824977c0daaa66",
      "44eb83d12b2949f59ed12b18743b9350",
      "3f68f30d0b6c40309f4087839a4a652b",
      "63665dfed314487b833869c15d3e81de",
      "ad997af490a7405a85293bcab6e37e0e",
      "02d7269d49284a9781ec66e079662e8a",
      "c149e156eaaa4b7081fd6dc0e366ac2c",
      "ea5cef09b1c247bca8b1eb73995224f9",
      "33fe6b74148a40ae8238fd7724e1e5e8",
      "501f9f4ce2d44b73b3e0ddf4f466528d",
      "e55559b48d794617b64746ac82f67eda",
      "87c9f94933b64a979dc600b0f612b302",
      "6b40efa203514759942185a5e245a5ea",
      "db701b496c084c5f857daa73c64821fe",
      "2fa58ff1ed7d4322a7640910144cb0c3",
      "e457d4efebb04b01a33c42f8499455b0",
      "ce60210224b545f484ac4c8332751ad0",
      "920b6ba54fd34cce8c85ac59aec6be43",
      "83b3597d54954093b3e3bfbdf974056e",
      "fc80b57dd61e497fa38d7ac3b682c907",
      "bac705fb58614b138dea0389b2ba9678",
      "1c90c54b05b74e0fa83cfc4b9175eec8",
      "a74401f00c6946ab909194d79edcffea",
      "ac481dd4fec6418083a79df45482d37d",
      "05f816a43cce4785a4b6e4a3d215b2f1",
      "b9242ac142004bcb9813ad26dacd9a58",
      "406093845f9d49a6b46b84f5681a7273",
      "e8950ad0af86461cb4fb920659ea1015",
      "41a92a4bd4834ace8e3ef0b86eff004e",
      "9cfdb8ca26e34a45a4c2d9c39c8aa8d6",
      "d7b81c400db94327b919a097ce61fa0e",
      "79480a4bb5c0426a8e87dcfeef5b1902",
      "ec292e15c1604c1ca88f3ceffecd1220",
      "1a661099a41f45e3a60255b739e91cb1",
      "8a1ae40dc2954cff894856da29fca50d",
      "62da33ceaa6d4e7fab72d509c0fb1a8c",
      "7b352afef52a45fa8d7e02c9fc844c58",
      "ab52bff45a6b4604b18fd7b4b0c18b02",
      "8240695b0f4f4889b16bf548a2be8c8d",
      "1776ec9dfb3944158e78a44c5d943765",
      "b39d49923a5f4333ada2fbf12c46f1de",
      "d7bd1c7db2014d458342c07af6b95754",
      "2797ed3b0e0f414db3419139101cb57c",
      "5d8181ef07ff45bd88384574fc244dcd",
      "7ace34cd70be478da40f823923c411c6",
      "7aeaea934df5444dafbd609e5c7488cb"
     ]
    },
    "id": "0ZxtN0f_1vew",
    "outputId": "89a2f536-5137-4306-96dc-133290f8d965"
   },
   "outputs": [],
   "source": [
    "device: str = \"cuda\"\n",
    "smol_model: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7vs-WGl7Kbt"
   },
   "source": [
    "# Loading Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "88b0e4c7741942a695ceaaeb44fcbaab",
      "594f58ed196249e58d8db83fc160da79",
      "32710a68d098466384d2060cdd5922e7",
      "7ec06d146ed044c3ba5e92aefa160e2e",
      "7a926a729a1c4bca8b9aca7b00281d9c",
      "30b6852a90404b07a82801d3f1a53efe",
      "685a004f816b4d1dbf7aa3d627d3a0f4",
      "b605f736ea4442a0b64578bbf5ba006a",
      "67caf97bf2e24a83b6ce36d2f2ae78f4",
      "35db379c7f324df4bd241c3527d1ff7c",
      "80a0a3be4d964b0ca882d4b19549c4b9",
      "87627f19dcc24f9693d7ac3746b155ee",
      "ce6a2936c76b430b9f387901967a938c",
      "f5ea77cdb48e42d583ef544e97812c02",
      "c6f2116ccaea407e8440be1ca06d50b4",
      "b090eb73aa3b4c0aad831e8611b26312",
      "95d86faadba14c2a9402505bce4f62cc",
      "693e375ceb4b49ccb31e37bf867bf1f5",
      "3a72016dcaa54503b6a0f45d9b0b5d2e",
      "71488013c286483d8491e5f7dac42bc7",
      "bef768dc2cab4de2bc36ece24544e02a",
      "5b0fc48f21ea4853a703ed3c89d2e82a",
      "987d30fa2ddf4cd2b96938a20dbde736",
      "1593fb0aba204b08900ffa2b6f3eb475",
      "426571abda2742ac8444f0a57709ae2d",
      "0d767eecc1c9441385d791f04663445f",
      "6bd48531941847b68296cffdae8e5cd9",
      "7629af154a6c4c9592e6f3ad0623c642",
      "f6c027919b8f40f3bc54a0310d8541fb",
      "1c42176024824a1e8c145751609acede",
      "a96bf267c731447a8d5ee779d5285aac",
      "78928d0eb3f843088855258f96c31f24",
      "783152e5ac694ad98559970262b61ea2",
      "d627393b04ca4505a2a56dcc1cf945b5",
      "66bd4e75462e4db8b604011a56fe8790",
      "dbfc627da13f4f6296eed068b30c9f66",
      "9b682319227e41839e1544929bf9cc5b",
      "cbf88445ed7d420e89e9e43c596e4d1e",
      "006c9792d8ae440f87e937e67041f2cd",
      "a826b40013de487385226f59279cc1e2",
      "30f544505ec84d0694f7443933f7d5bc",
      "c6715f4959314c2487886f4fb9302b06",
      "20f5e0741f104cf896be7561ecdd5b78",
      "51a82b50f63a4bf89198a83cd2433d69"
     ]
    },
    "id": "W4dpyN9J7XVA",
    "outputId": "4058904e-eba3-4805-aa01-69b386710b99"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you so much I am going to kill you and your entire family. You're going to rot in hell\", \"I hate you because I can't be with you. All I want is to hold you. I miss you and I love you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "MmScN5_093g4",
    "outputId": "02b78fb0-7ffb-4427-b885-756dcd61d5f6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_sentence_fragments(sentences, start_indices=None):\n",
    "    if not isinstance(sentences, list):\n",
    "      sentences = [sentences]\n",
    "\n",
    "    if isinstance(start_indices, int):\n",
    "      start_indices = [start_indices]\n",
    "\n",
    "    start_indices = start_indices or [1] * len(sentences)\n",
    "\n",
    "    assert len(start_indices) == len(sentences)\n",
    "\n",
    "    # words = sentence.split()\n",
    "    sentence_fragments = []\n",
    "    for start_index, sentence in zip(start_indices, sentences):\n",
    "      words = sentence.split()\n",
    "      assert start_index >= 0\n",
    "      sentence_fragments.extend([\n",
    "        words[start_index:i+1] for i in range(start_index, len(words))\n",
    "      ])\n",
    "\n",
    "    return sentence_fragments\n",
    "\n",
    "\n",
    "\n",
    "def process_sentence_sentiment_changes(sentences, start_indices=None):\n",
    "\n",
    "    sentence_fragments = get_sentence_fragments(sentences, start_indices)\n",
    "\n",
    "    sentiments = sentiment_pipeline([\n",
    "      ' '.join(fragment) for fragment in sentence_fragments\n",
    "    ])\n",
    "\n",
    "    def get_sentiment(result):\n",
    "      if result['label'] == 'POSITIVE':\n",
    "        return result['score']\n",
    "      return 1 - result['score']\n",
    "\n",
    "    return [\n",
    "      (fragment, get_sentiment(result))\n",
    "      for fragment, result in zip(sentence_fragments, sentiments)\n",
    "    ]\n",
    "\n",
    "\n",
    "def plot_sentiments(sentiments):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels, values = zip(*sentiments)\n",
    "    plt.plot(range(len(values)), values, marker='o', color='b', linestyle='-')\n",
    "    plt.xticks(range(len(values)), [frag[-1] for frag in labels], rotation=45)\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.xlabel('Phrases')\n",
    "    plt.ylabel('Sentiment Value')\n",
    "    plt.title('Word-by-Word Sentiment Changes')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test the script\n",
    "sentence = [\n",
    "  # \"Hello how are you I really hate you because you are beautiful\",\n",
    "  \"I hate you because I can't be with you. All I want is to hold you. I miss you and I love you\"\n",
    "]\n",
    "sentiments = process_sentence_sentiment_changes(sentence, 0)\n",
    "plot_sentiments(sentiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKvb2PvNWwsM"
   },
   "source": [
    "## Average Sentiment Across Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g0j96NtW0Ay"
   },
   "source": [
    "Given a set of prompts, and a number of times to complete each prompt, produce a graph averaging the sentiment after each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VolD239nPHls"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    (\"I hate you because you're the best person in the whole world\", 4),\n",
    "    (\"You're the worst because you smell like roses\", 3),\n",
    "    (\"Hello how are you I really hate you because you are beautiful\", 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDu7tStZCq7D"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_fragment_index_sentiments(sentences_with_start_indices):\n",
    "  \"\"\"\n",
    "  sentences: list of tuples with sentences and index where the completion begins\n",
    "  \"\"\"\n",
    "  sentences = [s for s, _ in sentences_with_start_indices]\n",
    "  start_indices = [i for _, i in sentences_with_start_indices]\n",
    "  sentiments = process_sentence_sentiment_changes(sentences, start_indices)\n",
    "  # sentiments_by_position = defaultdict(list)\n",
    "  # for sentence_fragment, value in sentiments:\n",
    "  #   sentiments_by_position[len(sentence_fragment)].append(value)\n",
    "\n",
    "  return pd.DataFrame([\n",
    "    {'index': len(sentence_fragment), 'sentiment': sentiment}\n",
    "    for sentence_fragment, sentiment in sentiments\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_-bW_awCq7D",
    "outputId": "ed46d2c4-b91f-40b2-d39c-c5bdc930b0d5"
   },
   "outputs": [],
   "source": [
    "compute_fragment_index_sentiments(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqa6y3_oQ4yQ",
    "outputId": "c1d2a7cc-63cd-40cb-b6d2-ce753817fbde"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "df = compute_fragment_index_sentiments(sentences)\n",
    "sns.lineplot(data=df, x='index', y='sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPss-lwFPkiz"
   },
   "outputs": [],
   "source": [
    "def plot_all_sentiments(data):\n",
    "\n",
    "  # Initialize lists to hold the average and standard deviation values\n",
    "  avg_values = []\n",
    "  std_values = []\n",
    "\n",
    "  # Calculate the average and standard deviation for each list in the dictionary\n",
    "  for key in sorted(data.keys()):\n",
    "      values = np.array(data[key])\n",
    "      avg_values.append(np.mean(values))\n",
    "      std_values.append(np.std(values))\n",
    "\n",
    "  # Create an array of x values corresponding to the keys in your dictionary\n",
    "  x_values = np.array(sorted(data.keys()))\n",
    "\n",
    "  # Plot the average values as a line graph\n",
    "  plt.errorbar(x_values, avg_values, yerr=std_values, fmt='-o')\n",
    "\n",
    "  # Adding title and labels\n",
    "  plt.title('Average Sentiment of Model Completions')\n",
    "  plt.xlabel('Index After Prompt')\n",
    "  plt.ylabel('Average Sentiment')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2xLu0QCGWd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def plot_all_sentiments_both(data_steered, data_unsteered):\n",
    "\n",
    "  # Initialize lists to hold the average and standard deviation values\n",
    "  avg_values_steered = []\n",
    "  std_values_steered = []\n",
    "\n",
    "  # Calculate the average and standard deviation for each list in the dictionary\n",
    "  for key in sorted(data_steered.keys()):\n",
    "      values_steered = np.array(data_steered[key])\n",
    "      avg_values_steered.append(np.mean(values_steered))\n",
    "      std_values_steered.append(np.std(values_steered))\n",
    "\n",
    "  # Create an array of x values corresponding to the keys in your dictionary\n",
    "  x_values_steered = np.array(sorted(data_steered.keys()))\n",
    "\n",
    "  # Plot the average values as a line graph\n",
    "  plt.errorbar(x_values_steered[:50], avg_values_steered[:50], yerr=std_values_steered[:50], fmt='--o', color='red', alpha=0.8, capsize=5)\n",
    "\n",
    "  # Repeat for unsteered!\n",
    "\n",
    "  # Initialize lists to hold the average and standard deviation values\n",
    "  avg_values_unsteered = []\n",
    "  std_values_unsteered = []\n",
    "\n",
    "  # Calculate the average and standard deviation for each list in the dictionary\n",
    "  for key in sorted(data_unsteered.keys()):\n",
    "      values_unsteered = np.array(data_unsteered[key])\n",
    "      avg_values_unsteered.append(np.mean(values_unsteered))\n",
    "      std_values_unsteered.append(np.std(values_unsteered))\n",
    "\n",
    "  # Create an array of x values corresponding to the keys in your dictionary\n",
    "  x_values_unsteered = np.array(sorted(data_unsteered.keys()))\n",
    "\n",
    "  # Plot the average values as a line graph\n",
    "  plt.errorbar(x_values_unsteered[:50], avg_values_unsteered[:50], yerr=std_values_unsteered[:50], fmt='--s', color='blue', alpha=0.8, capsize=5)\n",
    "\n",
    "\n",
    "  # Adding title and labels\n",
    "  plt.title('Average Sentiment of Model Completions')\n",
    "  plt.xlabel('Index After Prompt')\n",
    "  plt.ylabel('Average Sentiment')\n",
    "\n",
    "  current_time = datetime.now()\n",
    "  plt.savefig(f\"sentiments-{current_time}.pdf\", format=\"pdf\")\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFEpSdTbQpNq"
   },
   "outputs": [],
   "source": [
    "# plot_all_sentiments(all_sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTonzDgVRxBj"
   },
   "source": [
    "# Loading Toxic Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818,
     "referenced_widgets": [
      "76b49d4359d6417ea13657a5ee2138a8",
      "57728c2137774bb1b4d4fb1029f10be0",
      "f96a9ad7f57f4bcfab29870a0ac919a7",
      "5dbab0bf6eaf4b6dbe555379114f7f9f",
      "66b58ef1574b417bbefc75746437c462",
      "33114939e2fa4b1e945314de7106fca0",
      "8d72b04b12b446fe9f80aaf14284d932",
      "6fc17b4395764062911b85c90d6a3e89",
      "54edc83fcad54519b051a0e69bd988cb",
      "f1c8238c07f04aca9521eb6e9bac9206",
      "1da9663210d445798516a17a187f3b3f",
      "4c4f3dff08954fdca7ec809f55d9b32b",
      "f58393574e1845b894462d88f96ef201",
      "51f39462edcf4e6b95e85667c25bb2aa",
      "dad4e0807bb84a8ebcadf871432a212d",
      "2cbe988973874dc1bd25ed7ad06cb3fb",
      "bdc6c387427348d49e87bf4c8529f90d",
      "97880a3d773d42e69d9ff1b7b257eddb",
      "faf4b7a25b2c46138afa41b3b2180afc",
      "603c6d98877c408aa0b20b26d8428e8f",
      "d66d2aa7585d4f9097fdf39e2ddf6d13",
      "985606c441384366a5e61d94c6a222a1",
      "51b3e6e35da14e89a3251693683158ee",
      "ddbd4fa4974c4fcf98c9a4a8fac02795",
      "b474487996114d12bf4afb71614c9c8b",
      "8e7c08b4060d44eba5c5e185d4fa608d",
      "128eb07debf7445f8cad6930d4a24e24",
      "8a42ead019e148a3937fd16a9d1b706a",
      "b56d035a7cac4b2da78497c53ac976ec",
      "bb71f90b918c4ac683c0f1678aac7cdd",
      "8b5d19fc953946e5a92de38e97b4465c",
      "2b9a852a98a34e848124c1a504ca1a0d",
      "9ea7600ea9c1473aa06bdabc422a222e",
      "1242f55ca23c41e3b952d44909368ed0",
      "d595d55dfd93401793ce05644ddf6eec",
      "b65ca04efba44eaf84ab3ff008bacc79",
      "863292fe691542db8a9d42d109454ec1",
      "da17a2dc829a487084e6416027358d68",
      "71174d438dbb4fd5afdf072e89f7f10e",
      "53d2d33102884ef39955607a6237488c",
      "9baed87204194e7c860a18a79d2a406e",
      "89f7853a2b2a44aeac473190c89f4cb9",
      "5b0a182c08dd47eaaeeeaa2264aec61d",
      "aef04155696e40efa63b595cef943728",
      "2fa5a5a6737c4250905c43af6dba3b13",
      "7233949168c648a9bfe11625c9ed1da5",
      "da8ec9b98c3e489490cbe6fea57d716b",
      "499e2f70ba8c4b548a04eaa30bbb6518",
      "31349fbe9e1746378bec984d915ef9cc",
      "b18be3c5718647efba78c0a2013ef2e2",
      "3b59a1adc3064450a3559eb41e0d4952",
      "b56351994e8a4c0dba3910206a92ed8e",
      "6d73ba57633e4688aff75fb2f10cc46f",
      "954d48d293e549a98c074032a4b34441",
      "536a88d005e149789a79f18cdf8845d5",
      "7afc40a3397443c98a1d917da06550e6",
      "a70a319f8e264fd994204db6ff2554d1",
      "449135cac93f4bab829107063fa1514e",
      "48affc80130b4a799d3803f1668f1b25",
      "10858ffb58ab43a2b29e7a1a57844999",
      "24f7fd88328249cc80fae11a96866b9a",
      "d66ed54d3c57455993982f9336d6bd5b",
      "ea964b9144034655a8cbbbaa1661c58c",
      "f0771d476cf649f88d76e257f3886af3",
      "ef5ecd653dcb413bb96276ba477a595a",
      "a98cf5c4355547468e4e7181876a7a0f",
      "177edcf392344fe383fa8f3f81a97fb8",
      "3cecd466e6c6427d9d26c3cf7e2a62dc",
      "79888dfb972242d6b1b1e55f4272abc1",
      "5a73d2dc7dc847488b0ba9da78988c63",
      "2c30be89c73547729ad8d7ee91d17b6c",
      "ed793b92459b4bf3bc9814b8e826aa3f",
      "5febcfab00e74070b3212c06cbb50df5",
      "32de654090df45a9a564af409f5e343d",
      "e394ee50cc5c4222bb2032c079279643",
      "26a335d6b7e14d60b25936aef6926d35",
      "965a99f2002c4f1198883452d616883d",
      "3a7b33751c5f4e839c942a2c1a5e467c",
      "cc4110bc158145eb9cc669712b0d1a22",
      "c3afa92931c345059e254a17a7a9ceda",
      "9db6a5d7f2974d1883ad8f85400b5bcb",
      "d03067eb115c4ab2a812493573eb6c63",
      "91487e52396d488fad2dc4f311909589",
      "777a2b5e9a044f5fbb366188f600ad53",
      "c4bbf61cd7d7494f822fb93c3a56e100",
      "6a9460d73bfc4cbcb50f24513b9b0a59",
      "b0b87e495d25412d935563c9449d2938",
      "e48e8a97fe6d43abad824eef142cc465"
     ]
    },
    "id": "K3oebTXaRzWG",
    "outputId": "2aaf9e10-b0ca-4269-9f85-3fc2fae61f17"
   },
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "toxic_dataset = load_dataset(\"vmalperovich/toxic_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsSbRKFHTNHz"
   },
   "outputs": [],
   "source": [
    "shuffled_dataset = toxic_dataset.shuffle(seed=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iKLHxvYTYsd",
    "outputId": "e0e11379-d73b-4a50-a7b5-990575ebdcaf"
   },
   "outputs": [],
   "source": [
    "shuffled_dataset['train'][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HrxBHqRU2-l"
   },
   "outputs": [],
   "source": [
    "filtered_toxic_ds = shuffled_dataset['train'].filter(lambda example: 1 in example['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXNB5ZYVWBx",
    "outputId": "f4151842-9757-4c22-e22d-fd53d5cd879a"
   },
   "outputs": [],
   "source": [
    "filtered_toxic_ds[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJrj8YT8R1hG"
   },
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = 'datasets/goose_training_subset.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  goose_dict = json.load(file)\n",
    "\n",
    "goose_text = []\n",
    "for key, value in goose_dict.items():\n",
    "  goose_text.extend(value)\n",
    "\n",
    "# Create the baseline dataset\n",
    "\n",
    "def read_all_text_files(directory):\n",
    "    # List to hold the contents of all files\n",
    "    contents_list = []\n",
    "\n",
    "    # List all files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if file is a text file\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct full file path\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Open the file and read the contents\n",
    "            with open(filepath, 'r') as f:\n",
    "                contents = f.read()\n",
    "\n",
    "            # Add the file contents to the list\n",
    "            contents_list.append(contents)\n",
    "\n",
    "    return contents_list\n",
    "\n",
    "training_subset = read_all_text_files('datasets/urlsf_subset01-1_data') + read_all_text_files('datasets/urlsf_subset01-182_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I35xF1PNQ7y-"
   },
   "source": [
    "Do this once loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBX4DcKxQ6oy"
   },
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "file_path = 'loving_500.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_loving = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUIN1z4ECq7E"
   },
   "outputs": [],
   "source": [
    "with open('datasets/fantasy_200.json', 'r') as file:\n",
    "    fantasy_ds = json.load(file)\n",
    "\n",
    "with open('datasets/scifi_200.json', 'r') as file:\n",
    "    scifi_ds = json.load(file)\n",
    "\n",
    "with open('datasets/sports_200.json', 'r') as file:\n",
    "    sports_ds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbJCUK8uCq7E"
   },
   "outputs": [],
   "source": [
    "shakespeare_ds = load_dataset(\"tiny_shakespeare\")\n",
    "shakespeare_text = shakespeare_ds['train']['text'][0]\n",
    "sample_size = 100\n",
    "words = shakespeare_text.split()\n",
    "shakespeare_ds = [' '.join(words[i:i+sample_size]) for i in range(0, len(words), sample_size)][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQN4adzcCq7E",
    "outputId": "6669c887-1a35-41ca-e957-d7efbcc1f6fb"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "fantasy_words = set(' '.join(fantasy_ds).split()) - set(' '.join(scifi_ds).split()) - set(' '.join(sports_ds).split())\n",
    "fantasy_words = {stemmer.stem(word) for word in fantasy_words}\n",
    "scifi_words = set(' '.join(scifi_ds).split()) - set(' '.join(fantasy_ds).split()) - set(' '.join(sports_ds).split())\n",
    "scifi_words = {stemmer.stem(word) for word in scifi_words}\n",
    "sports_words = set(' '.join(sports_ds).split()) - set(' '.join(fantasy_ds).split()) - set(' '.join(scifi_ds).split())\n",
    "sports_words = {stemmer.stem(word) for word in sports_words}\n",
    "shakespeare_words = set(' '.join(shakespeare_ds).split()) - set(' '.join(fantasy_ds).split()) - set(' '.join(scifi_ds).split()) - set(' '.join(sports_ds).split()) - set(' '.join(training_subset).split())\n",
    "shakespeare_words = {stemmer.stem(word) for word in shakespeare_words}\n",
    "\n",
    "\n",
    "list(fantasy_words)[:5], list(scifi_words)[:5], list(sports_words)[:5], list(shakespeare_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeH6Nb4cCq7E",
    "outputId": "d6d707ed-c842-48c2-e82e-17ba8649da9a"
   },
   "outputs": [],
   "source": [
    "len(fantasy_words), len(shakespeare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTFbFwGtCq7E"
   },
   "outputs": [],
   "source": [
    "def get_genre_frequencies(text):\n",
    "    words = {stemmer.stem(w) for w in set(text.split())}\n",
    "    fantasy_freq = len(words & fantasy_words) / len(words)\n",
    "    scifi_freq = len(words & scifi_words) / len(words)\n",
    "    sports_freq = len(words & sports_words) / len(words)\n",
    "    return fantasy_freq, scifi_freq, sports_freq\n",
    "\n",
    "\n",
    "def get_genre_frequency_changes(text):\n",
    "    fantasy_count = 0\n",
    "    scifi_count = 0\n",
    "    sports_count = 0\n",
    "    shakes_count = 0\n",
    "    fantasy_array = []\n",
    "    scifi_array = []\n",
    "    sports_array = []\n",
    "    shakes_array = []\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in fantasy_words:\n",
    "            fantasy_count += 1\n",
    "        if word in scifi_words:\n",
    "            scifi_count += 1\n",
    "        if word in sports_words:\n",
    "            sports_count += 1\n",
    "        if word in shakespeare_words:\n",
    "            shakes_count += 1\n",
    "        fantasy_array.append(fantasy_count / (i + 1))\n",
    "        scifi_array.append(scifi_count / (i + 1))\n",
    "        sports_array.append(sports_count / (i + 1))\n",
    "        shakes_array.append(shakes_count / (i + 1))\n",
    "\n",
    "    return fantasy_array, scifi_array, sports_array, shakes_array\n",
    "\n",
    "\n",
    "def get_genres_freq_df(texts, start_indices=None):\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "    start_indices = start_indices or [0] * len(texts)\n",
    "    if isinstance(start_indices, int):\n",
    "        start_indices = [start_indices]\n",
    "    items = []\n",
    "    for text, start_index in zip(texts, start_indices):\n",
    "        for i, (v_f, v_sc, v_sp, v_sh) in enumerate(zip(*get_genre_frequency_changes(text))):\n",
    "            items.append({\n",
    "                'position': i - start_index,\n",
    "                'fantasy': v_f,\n",
    "                'scifi': v_sc,\n",
    "                'sports': v_sp,\n",
    "                'shakespeare': v_sh\n",
    "            })\n",
    "    return pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B998bJpTCq7E",
    "outputId": "eb04264c-0782-4dc5-f66d-38ec19a434f3"
   },
   "outputs": [],
   "source": [
    "fantasy_genres_freq_df = get_genres_freq_df(fantasy_ds)\n",
    "fantasy_genres_freq_df['dataset'] = 'fantasy'\n",
    "scifi_genres_freq_df = get_genres_freq_df(scifi_ds)\n",
    "scifi_genres_freq_df['dataset'] = 'scifi'\n",
    "sports_genres_freq_df = get_genres_freq_df(sports_ds)\n",
    "sports_genres_freq_df['dataset'] = 'sports'\n",
    "shakes_freq_df = get_genres_freq_df(shakespeare_ds)\n",
    "shakes_freq_df['dataset'] = 'shakespeare'\n",
    "\n",
    "genres_freq_df = pd.concat([fantasy_genres_freq_df, scifi_genres_freq_df, sports_genres_freq_df, shakes_freq_df])\n",
    "genres_freq_df = genres_freq_df.sort_values(by='position')\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(data=genres_freq_df, x='position', y='fantasy', hue='dataset', ax=axs[0])\n",
    "sns.lineplot(data=genres_freq_df, x='position', y='scifi', hue='dataset', ax=axs[1])\n",
    "sns.lineplot(data=genres_freq_df, x='position', y='sports', hue='dataset', ax=axs[2])\n",
    "sns.lineplot(data=genres_freq_df, x='position', y='shakespeare', hue='dataset', ax=axs[3])\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y24BhZyRwSQ"
   },
   "source": [
    "# Loading Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2HSiZujCq7E"
   },
   "outputs": [],
   "source": [
    "short_training_subset = [stringo for stringo in training_subset if len(smol_model.tokenizer(stringo)[\"input_ids\"]) < 1000]\n",
    "tiny_training_subset = [stringo for stringo in training_subset if len(smol_model.tokenizer(stringo)[\"input_ids\"]) < 500]\n",
    "len(tiny_training_subset)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def completion_df_to_list(df):\n",
    "    def create_tuple(row):\n",
    "      # Combine prompt and completion\n",
    "      combined_text = row['prompts'] + ' ' + row['completions']\n",
    "      # Count the number of words in the prompt\n",
    "      num_words = len(row['prompts'].split())\n",
    "      return (combined_text, num_words)\n",
    "\n",
    "    # Apply the function to each row in the DataFrame to create the list of tuples\n",
    "    result = df.apply(create_tuple, axis=1).tolist()\n",
    "    return result\n",
    "\n",
    "def first_half_string(s):\n",
    "    # Split the string into words\n",
    "    words = s.split()\n",
    "\n",
    "    # Calculate the index to split the words list in half\n",
    "    half_index = len(words) // 2\n",
    "\n",
    "    # Take the first half of the words\n",
    "    first_half_words = words[:half_index]\n",
    "\n",
    "    # Combine these words to form a string\n",
    "    result = ' '.join(first_half_words)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_last_if_even(lst):\n",
    "    \"\"\"\n",
    "    Remove the last element from a list if the list has even length.\n",
    "\n",
    "    :param lst: List from which to remove the last element if the list has even length.\n",
    "    :type lst: list\n",
    "    :return: Modified list.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if len(lst) % 2 == 0:  # Check if the length of the list is even\n",
    "        return lst[:-1]  # Return the list excluding the last element\n",
    "    return lst\n",
    "\n",
    "input_dataset = fantasy_ds\n",
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "prompt_batch = remove_last_if_even(small_data)\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9e4efENCq7F"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=scifi_ds,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "fantasy_to_scifi_df = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=42,\n",
    "    **default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW1JLxdwCq7F",
    "outputId": "15cb3c14-431f-4a20-b1ce-aa8e6dd74c2a"
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "for prompt, contiuations in zip(fantasy_to_scifi_df.prompts[:i], fantasy_to_scifi_df.completions[:i]):\n",
    "    print(r'\\textbf{' + prompt + '}' + contiuations + r'\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7EFmNsfCq7F"
   },
   "outputs": [],
   "source": [
    "input_dataset = sports_ds\n",
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "prompt_batch = remove_last_if_even(small_data)\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=fantasy_ds,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "sports_to_fantasy_df = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=42,\n",
    "    **default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89QO-ATTCq7F",
    "outputId": "a17bc135-59a4-470f-a3b2-28f3572fd4a0"
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "for prompt, contiuations in zip(sports_to_fantasy_df.prompts[:i], sports_to_fantasy_df.completions[:i]):\n",
    "    print(r'\\textbf{' + prompt + '}' + contiuations + r'\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxubHeD4Cq7F"
   },
   "outputs": [],
   "source": [
    "input_dataset = scifi_ds\n",
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "prompt_batch = remove_last_if_even(small_data)\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=sports_ds,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "sci_fi_to_sports_df = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=42,\n",
    "    **default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_MeVLxuCq7F",
    "outputId": "120a1e42-4e54-44f4-90cb-b5a0bcf35479"
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "for prompt, contiuations in zip(sci_fi_to_sports_df.prompts[:i], sci_fi_to_sports_df.completions[:i]):\n",
    "    print(r'\\textbf{' + prompt + '}' + contiuations + r'\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK8j6JWACq7F"
   },
   "outputs": [],
   "source": [
    "# input_dataset = tiny_training_subset[:200]\n",
    "# halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "# small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "# prompt_batch = remove_last_if_even(small_data)\n",
    "# default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "# from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "# activation_addition_dataset = [ActivationAdditionDataset(\n",
    "#     coeff=60,\n",
    "#     act_name=2,\n",
    "#     prompt=shakespeare_ds[:500],\n",
    "#     from_dataset=True,\n",
    "#     use_all_activations=True,\n",
    "#     prompt_2=tiny_training_subset[:200],\n",
    "#     from_pca=False,\n",
    "#     from_difference=True,\n",
    "# )]\n",
    "\n",
    "# training_to_shakespeare_df = gen_using_activation_additions(\n",
    "#     prompt_batch=prompt_batch,\n",
    "#     model=smol_model,\n",
    "#     activation_additions=activation_addition_dataset,\n",
    "#     addition_location=\"front\",\n",
    "#     seed=42,\n",
    "#     **default_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uupn3nlaCq7F"
   },
   "outputs": [],
   "source": [
    "# i = 5\n",
    "# for prompt, contiuations in zip(training_to_shakespeare_df.prompts[:i], training_to_shakespeare_df.completions[:i]):\n",
    "#     print(r'\\textbf{' + prompt + '}' + contiuations + r'\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3J5PEjoHCq7F",
    "outputId": "d6a7fbdf-31e9-48cd-e862-e020ef9ef01a"
   },
   "outputs": [],
   "source": [
    "generated_stories = list(fantasy_to_scifi_df.prompts + fantasy_to_scifi_df.completions)\n",
    "steering_start_indices = [len(prompt.split()) for prompt in fantasy_to_scifi_df.prompts]\n",
    "fantasy_genres_freq_df = get_genres_freq_df(generated_stories, steering_start_indices)\n",
    "\n",
    "# genres_freq_df = pd.concat([fantasy_genres_freq_df, scifi_genres_freq_df, sports_genres_freq_df])\n",
    "\n",
    "from cProfile import label\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "sns.set()\n",
    "plt.rcParams.update({\n",
    "    'font.family':'serif',\n",
    "    \"text.usetex\": False,\n",
    "    'savefig.facecolor': 'white',\n",
    "})\n",
    "\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=12)\n",
    "# plt.rc('xtick', labelsize=16)\n",
    "# plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=11)\n",
    "# plt.rc('figure', titlesize=20)\n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "# sns.lineplot(data=fantasy_genres_freq_df, x='position', y='fantasy', ax=axs[0], label='fantasy')\n",
    "# sns.lineplot(data=fantasy_genres_freq_df, x='position', y='scifi', ax=axs[0], label='scifi')\n",
    "axs[0].vlines(0, 0, 1, color='black', linestyles='dashed', alpha=0.25)\n",
    "for dataset in ['fantasy', 'scifi', 'sports']:\n",
    "    sns.lineplot(data=fantasy_genres_freq_df, x='position', y=dataset, ax=axs[0])\n",
    "axs[0].set_title('Fantasy-to-Scifi Steering')\n",
    "axs[0].set_xlabel('Relative Word Index')\n",
    "axs[0].set_ylabel('Genre Word Frequency')\n",
    "axs[0].set_xlim([-50, 75])\n",
    "axs[0].set_ylim([.0, .2])\n",
    "clear_output()\n",
    "\n",
    "genered_stories = list(sports_to_fantasy_df.prompts + sports_to_fantasy_df.completions)\n",
    "steering_start_indices = [len(prompt.split()) for prompt in sports_to_fantasy_df.prompts]\n",
    "sports_genres_freq_df = get_genres_freq_df(genered_stories, steering_start_indices)\n",
    "\n",
    "# sns.lineplot(data=sports_genres_freq_df, x='position', y='sports', ax=axs[1], label='sports')\n",
    "# sns.lineplot(data=sports_genres_freq_df, x='position', y='fantasy', ax=axs[1], label='fantasy')\n",
    "axs[1].vlines(0, 0, 1, color='black', linestyles='dashed', alpha=0.25)\n",
    "for dataset in ['fantasy', 'scifi', 'sports']:\n",
    "    sns.lineplot(data=sports_genres_freq_df, x='position', y=dataset, ax=axs[1])\n",
    "axs[1].set_title('Sports-to-Fantasy Steering')\n",
    "axs[1].set_xlabel('Relative Word Index')\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_xlim([-50, 75])\n",
    "axs[1].set_ylim([.0, .25])\n",
    "clear_output()\n",
    "\n",
    "generated_stories = list(sci_fi_to_sports_df.prompts + sci_fi_to_sports_df.completions)\n",
    "steering_start_indices = [len(prompt.split()) for prompt in sci_fi_to_sports_df.prompts]\n",
    "scifi_genres_freq_df = get_genres_freq_df(generated_stories, steering_start_indices)\n",
    "\n",
    "# sns.lineplot(data=scifi_genres_freq_df, x='position', y='scifi', ax=axs[2], label='scifi')\n",
    "# sns.lineplot(data=scifi_genres_freq_df, x='position', y='sports', ax=axs[2], label='sports')\n",
    "\n",
    "legend_labels = {\n",
    "    'fantasy': 'Fantasy\\nWords',\n",
    "    'scifi': 'Sci-fi\\nWords',\n",
    "    'sports': 'Sports\\nWords',\n",
    "}\n",
    "axs[2].vlines(0, 0, 1, color='black', linestyles='dashed', alpha=0.25, label='Steering\\nStart')\n",
    "\n",
    "for dataset in ['fantasy', 'scifi', 'sports']:\n",
    "    sns.lineplot(data=scifi_genres_freq_df, x='position', y=dataset, ax=axs[2], label=legend_labels[dataset])\n",
    "\n",
    "axs[2].set_title('Scifi-to-Sports Steering')\n",
    "axs[2].set_xlabel('Relative Word Index')\n",
    "axs[2].set_ylabel('')\n",
    "axs[2].set_xlim([-50, 75])\n",
    "axs[2].set_ylim([.0, .25])\n",
    "axs[2].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "\n",
    "# generated_stories = list(training_to_shakespeare_df.prompts + training_to_shakespeare_df.completions)\n",
    "# steering_start_indices = [len(prompt.split()) for prompt in training_to_shakespeare_df.prompts]\n",
    "# shakes_freq_df = get_genres_freq_df(generated_stories, steering_start_indices)\n",
    "# # sns.lineplot(data=shakes_freq_df, x='position', y='sports', ax=axs[3], label='sports')\n",
    "# # sns.lineplot(data=shakes_freq_df, x='position', y='shakespeare', ax=axs[3], label='shakespeare')\n",
    "\n",
    "# for dataset in ['fantasy', 'scifi', 'sports', 'shakespeare']:\n",
    "#     sns.lineplot(data=shakes_freq_df, x='position', y=dataset, ax=axs[3], label=dataset)\n",
    "# axs[3].set_title('Training Data with Shakespeare Steering')\n",
    "# axs[3].set_xlabel('Relative Word Index')\n",
    "# axs[3].set_ylabel('')\n",
    "# axs[3].set_xlim([-50, 75])\n",
    "# # set legend to the right of the plot\n",
    "# axs[3].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Relative Word Index')\n",
    "plt.savefig(f\"genres-steering.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYsWc2xuWmQs"
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14nMiEfGQ9qz"
   },
   "source": [
    "# Finding good coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJJ-nvWEQ9Qh",
    "outputId": "0346effc-aa1b-4c5b-e4c9-24653eee1d15"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=short_training_subset[:390],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'model': smol_model}\n",
    "\n",
    "print_n_comparisons(prompt=\"I hate you because\",\n",
    "                    tokens_to_generate=40, activation_additions=activation_addition_dataset,\n",
    "                    num_comparisons=4, seed=0, **default_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGyhdYztx4VO"
   },
   "source": [
    "# Doing Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gUWfEsKx_Pa"
   },
   "source": [
    "Once we have done our hyper-parameter search, we can do apply the sentiment classifier stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKQieNE2WNvF"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=short_training_subset[:390],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate':80}\n",
    "\n",
    "loving_df = gen_using_activation_additions(\n",
    "    prompt_batch = [\"I hate you because\"] * 8,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=0,\n",
    "    **default_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bZgTw4gCq7G"
   },
   "outputs": [],
   "source": [
    "unsteered_df_hate = gen_using_model(\n",
    "    model=smol_model,\n",
    "    prompt_batch = [\"I hate you because\"] * 8,\n",
    "    seed = 0,\n",
    "    **default_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S99wZ0LMCq7G"
   },
   "outputs": [],
   "source": [
    "unsteered_df_hate = gen_using_model(\n",
    "    model=smol_model,\n",
    "    prompt_batch = [\"I hate you because\", \"I like you because you\"],\n",
    "    seed = 1,\n",
    "    **default_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MwiRY9UCq7G",
    "outputId": "a2bfd840-2948-4dd3-d96a-924f09e187e7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# sentence = \"I hate you because\" + loving_df['completions'][3]\n",
    "start_index = 3  # example start index\n",
    "df = compute_fragment_index_sentiments([\n",
    "    (f'I hate you because{completion}', start_index)\n",
    "    for completion in loving_df['completions']\n",
    "])\n",
    "sns.lineplot(data=df, x='index', y='sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "O44VX13OyU18",
    "outputId": "f1b84099-3e3e-4bb9-947d-c9ddd19d22f1"
   },
   "outputs": [],
   "source": [
    "# Test the script\n",
    "sentence = \"I hate you because\" + loving_df['completions'][3]\n",
    "start_index = 3  # example start index\n",
    "sentiments = process_sentence(sentence, start_index)\n",
    "plot_sentiments(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tJSeYpP0LgO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG3JvulJ1Gjf"
   },
   "source": [
    "# Trying with GPT-2 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc_2w3vX156Z"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbe2TRHE1SBy"
   },
   "outputs": [],
   "source": [
    "# from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "# activation_addition_dataset = [ActivationAdditionDataset(\n",
    "#     coeff=60,\n",
    "#     act_name=14,\n",
    "#     prompt=dataset_loving,\n",
    "#     from_dataset=True,\n",
    "#     use_all_activations=True,\n",
    "#     prompt_2=tiny_training_subset[:200],\n",
    "#     from_pca=False,\n",
    "#     from_difference=True,\n",
    "# )]\n",
    "\n",
    "# default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'model': model}\n",
    "\n",
    "# print_n_comparisons(prompt=\"I hate you because\",\n",
    "#                     tokens_to_generate=80, activation_additions=activation_addition_dataset,\n",
    "#                     num_comparisons=8, seed=0, **default_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WK7Gicb64NrS"
   },
   "outputs": [],
   "source": [
    "# from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "# activation_addition_dataset = [ActivationAdditionDataset(\n",
    "#     coeff=120,\n",
    "#     act_name=14,\n",
    "#     prompt=dataset_loving,\n",
    "#     from_dataset=True,\n",
    "#     use_all_activations=True,\n",
    "#     prompt_2=tiny_training_subset[:200],\n",
    "#     from_pca=False,\n",
    "#     from_difference=True,\n",
    "# )]\n",
    "\n",
    "# default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'model': model}\n",
    "\n",
    "# print_n_comparisons(prompt=\"I hate you because\",\n",
    "#                     tokens_to_generate=80, activation_additions=activation_addition_dataset,\n",
    "#                     num_comparisons=8, seed=0, **default_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV7AKWhh4RAx"
   },
   "outputs": [],
   "source": [
    "# from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "# activation_addition_dataset = [ActivationAdditionDataset(\n",
    "#     coeff=270,\n",
    "#     act_name=26,\n",
    "#     prompt=dataset_loving,\n",
    "#     from_dataset=True,\n",
    "#     use_all_activations=True,\n",
    "#     prompt_2=tiny_training_subset[:200],\n",
    "#     from_pca=False,\n",
    "#     from_difference=True,\n",
    "# )]\n",
    "\n",
    "# default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'model': model}\n",
    "\n",
    "# print_n_comparisons(prompt=\"I hate you because\",\n",
    "#                     tokens_to_generate=80, activation_additions=activation_addition_dataset,\n",
    "#                     num_comparisons=8, seed=0, **default_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv2_pYZH5xBl"
   },
   "outputs": [],
   "source": [
    "# from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "# activation_addition_dataset = [ActivationAdditionDataset(\n",
    "#     coeff=90,\n",
    "#     act_name=7,\n",
    "#     prompt=dataset_loving,\n",
    "#     from_dataset=True,\n",
    "#     use_all_activations=True,\n",
    "#     prompt_2=tiny_training_subset[:200],\n",
    "#     from_pca=False,\n",
    "#     from_difference=True,\n",
    "# )]\n",
    "\n",
    "# default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'model': model}\n",
    "\n",
    "# print_n_comparisons(prompt=\"I hate you because\",\n",
    "#                     tokens_to_generate=80, activation_additions=activation_addition_dataset,\n",
    "#                     num_comparisons=8, seed=0, **default_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6sWok6gfG3S"
   },
   "source": [
    "# Using dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veNQqt0mfGD1"
   },
   "outputs": [],
   "source": [
    "# from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "# activation_addition_dataset = [ActivationAdditionDataset(\n",
    "#     coeff=90,\n",
    "#     act_name=7,\n",
    "#     prompt=dataset_loving,\n",
    "#     from_dataset=True,\n",
    "#     use_all_activations=True,\n",
    "#     prompt_2=tiny_training_subset[:200],\n",
    "#     from_pca=False,\n",
    "#     from_difference=True,\n",
    "# )]\n",
    "\n",
    "# default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3}\n",
    "\n",
    "\n",
    "# loving_df = gen_using_activation_additions(\n",
    "#     prompt_batch = [\"I hate you because\"] * 8,\n",
    "#     model=model,\n",
    "#     activation_additions=activation_addition_dataset,\n",
    "#     addition_location=\"front\",\n",
    "#     seed=0,\n",
    "#     **default_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsaWOxNFhAoS"
   },
   "outputs": [],
   "source": [
    "# squanch = completion_df_to_list(loving_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVeajA-uhLF3"
   },
   "outputs": [],
   "source": [
    "# squanch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM4hQhoBa1oC"
   },
   "source": [
    "# Impact of Steering on Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BorZ1KB5bSS9"
   },
   "source": [
    "Given a toxic dataset, split each input in half. Continue from halfway, and generate n number os completions both with and without steering. Then process these to get the average sentiment graphs.\n",
    "\n",
    "Requires hyper-parameters to have already been found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K4z6fXMcsIq"
   },
   "outputs": [],
   "source": [
    "input_dataset = filtered_toxic_ds[0:100]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgq8mid9pD-Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omEx7QFzbRXj"
   },
   "outputs": [],
   "source": [
    "def steering_sentiment_experiment(\n",
    "  input_dataset,\n",
    "  n_completions,\n",
    "  model,\n",
    "  activation_addition_dataset,\n",
    "  addition_location,\n",
    "  seed,\n",
    "  default_kwargs\n",
    "):\n",
    "  halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "  small_data = [s for s in halfway_data if len(model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "\n",
    "  small_data = remove_last_if_even(small_data)\n",
    "\n",
    "  prompt_batch = small_data\n",
    "\n",
    "\n",
    "\n",
    "  # Take the inputs, and run them through the model\n",
    "  for i in range(n_completions):\n",
    "    steered_df = gen_using_activation_additions(\n",
    "      prompt_batch = prompt_batch,\n",
    "      model=model,\n",
    "      activation_additions=activation_addition_dataset,\n",
    "      addition_location=\"front\",\n",
    "      seed=i,\n",
    "      **default_kwargs\n",
    "      )\n",
    "\n",
    "    if i == 0:\n",
    "      whole_df = steered_df\n",
    "    else:\n",
    "      steered_df = pd.concat([whole_df, steered_df], ignore_index=True)\n",
    "\n",
    "  # Repeat for unsteered stuff!\n",
    "  for i in range(n_completions):\n",
    "    unsteered_df = gen_using_model(\n",
    "      model= smol_model,\n",
    "      prompt_batch = prompt_batch,\n",
    "      seed = 0,\n",
    "      **default_kwargs,\n",
    "    )\n",
    "\n",
    "    if i == 0:\n",
    "      whole_unsteered_df = unsteered_df\n",
    "    else:\n",
    "      unsteered_df = pd.concat([whole_unsteered_df, unsteered_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Do the plotting experiment on the df\n",
    "# sentiments_list_steered = completion_df_to_list(whole_df)\n",
    "# all_sentiments_dict_steered = all_sentiments(sentiments_list_steered)\n",
    "\n",
    "# sentiments_list_unsteered = completion_df_to_list(whole_unsteered_df)\n",
    "# all_sentiments_dict_unsteered = all_sentiments(sentiments_list_unsteered)\n",
    "\n",
    "# plot_all_sentiments_both(all_sentiments_dict_steered, all_sentiments_dict_unsteered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6fQ9amHCq7H"
   },
   "outputs": [],
   "source": [
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "prompt_batch = remove_last_if_even(small_data)\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYGv22inCq7H",
    "outputId": "018c9f9a-5749-41b1-f59f-60799e77aff4"
   },
   "outputs": [],
   "source": [
    "prompt_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PmQ1h7VCq7H"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=65,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "loving_steered_df = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=27,\n",
    "    **default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha9y_M4KCq7H"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.prompt_utils import ActivationAddition, get_x_vector\n",
    "\n",
    "x_vector = get_x_vector(\n",
    "    prompt1='Love',\n",
    "    prompt2='Hate',\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    model=smol_model,\n",
    ")\n",
    "\n",
    "activation_addition_dataset = [ActivationAddition(\n",
    "    prompt='I love you',\n",
    "    coeff=5,\n",
    "    act_name=2,\n",
    "    # use_all_activations=True,\n",
    "    # prompt_2=tiny_training_subset[:200],\n",
    "    # from_pca=False,\n",
    "    # from_difference=True,\n",
    ")]\n",
    "\n",
    "turner_steered_df = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=27,\n",
    "    **default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHfdvW_JCq7H",
    "outputId": "5e48960c-4fa4-4c93-88d7-732fc367af95"
   },
   "outputs": [],
   "source": [
    "turner_steered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqcKWPPDCq7H",
    "outputId": "c3f25131-b4a7-40d8-a5b5-7a5b94e88109"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.prompt_utils import ActivationAddition, get_x_vector\n",
    "\n",
    "x_vector = get_x_vector(\n",
    "    prompt1='Love',\n",
    "    prompt2='Hate',\n",
    "    coeff=5,\n",
    "    act_name=2,\n",
    "    model=smol_model,\n",
    ")\n",
    "\n",
    "# activation_addition_dataset = [ActivationAddition(\n",
    "#     prompt='Love',\n",
    "#     coeff=60,\n",
    "#     act_name=2,\n",
    "#     # use_all_activations=True,\n",
    "#     # prompt_2=tiny_training_subset[:200],\n",
    "#     # from_pca=False,\n",
    "#     # from_difference=True,\n",
    "# )]\n",
    "\n",
    "turner_steered_df_2 = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=[*x_vector],\n",
    "    addition_location=\"front\",\n",
    "    seed=27,\n",
    "    **default_kwargs\n",
    ")\n",
    "\n",
    "turner_steered_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUQQFM8-Cq7H"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_latex_random_continuation(df, n=5, seed=0):\n",
    "    print(r'\\begin{itemize}')\n",
    "    random.seed(seed)\n",
    "    for prompt, continuation in random.sample(list(zip(df.prompts, df.completions)), n):\n",
    "        print(r'\\item \\textbf{' + prompt + r'}' + continuation)\n",
    "    print(r'\\end{itemize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WdDJljCCq7H",
    "outputId": "804aaed9-12cc-4b97-d185-c0cb01bdcb0c"
   },
   "outputs": [],
   "source": [
    "print_latex_random_continuation(turner_steered_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYndXh3NCq7H"
   },
   "outputs": [],
   "source": [
    "df_sentiments_turner_steered_2 = compute_fragment_index_sentiments([(completion, 0) for completion in turner_steered_df_2.completions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiwA79v1Cq7H"
   },
   "outputs": [],
   "source": [
    "df_sentiments_turner_steered = compute_fragment_index_sentiments([(completion, 0) for completion in turner_steered_df.completions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeiZIJLRCq7H",
    "outputId": "a9b5f768-3884-48ec-c3d6-d1d75095177e"
   },
   "outputs": [],
   "source": [
    "loving_steered_df.prompts[0], loving_steered_df.completions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIICRfu7Cq7I"
   },
   "outputs": [],
   "source": [
    "loving_steered_df['start_index'] = loving_steered_df['prompts'].apply(lambda x: len(x.split()))\n",
    "loving_steered_df['full_sentence'] = loving_steered_df['prompts'] + loving_steered_df['completions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YkVCz8dCq7I"
   },
   "outputs": [],
   "source": [
    "df_sentiments_steered = compute_fragment_index_sentiments([(completion, 0) for completion in loving_steered_df.completions])\n",
    "# df_sentiments_steered = compute_fragment_index_sentiments([(completion, 0) for completion in steered_df.full_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxnqnVkfCq7I"
   },
   "outputs": [],
   "source": [
    "unsteered_df = gen_using_model(\n",
    "    model= smol_model,\n",
    "    prompt_batch = prompt_batch,\n",
    "    seed = 0,\n",
    "    **default_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-4xRGHeCq7I",
    "outputId": "b4012aa4-ceae-4795-a661-2fab8ea93105"
   },
   "outputs": [],
   "source": [
    "unsteered_df['full_sentence'] = unsteered_df['prompts'] + unsteered_df['completions']\n",
    "unsteered_df['start_index'] = unsteered_df['prompts'].apply(lambda x: len(x.split()))\n",
    "unsteered_df['full_sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U92qrk79Cq7I"
   },
   "outputs": [],
   "source": [
    "df_sentiments_unsteered = compute_fragment_index_sentiments([(completion, 0) for completion in unsteered_df.completions])\n",
    "# df_sentiments_unsteered = compute_fragment_index_sentiments([(completion, 0) for completion in unsteered_df.full_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0R-JcYTmCq7I"
   },
   "outputs": [],
   "source": [
    "civil_comments = load_dataset(\"civil_comments\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zPpymE8Cq7I",
    "outputId": "4a35b411-620e-4b2b-ca34-92ef7b380967"
   },
   "outputs": [],
   "source": [
    "civil_comments[10], civil_comments[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeLcbVzmCq7I",
    "outputId": "db819f7b-8b28-4b0f-d282-f8bf3ecb25c1"
   },
   "outputs": [],
   "source": [
    "feats = {k for k in civil_comments[0] if k != 'text'}\n",
    "not_toxic_comments = [\n",
    "    comment['text'] for comment in civil_comments\n",
    "    if len(comment['text']) < 500 and all(comment[feat] == 0 for feat in feats)\n",
    "]\n",
    "not_toxic_comments[:5], len(not_toxic_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz0UOfXyCq7I",
    "outputId": "6fa9ceb2-ab6a-4026-fcf6-34cf4ffa40a4"
   },
   "outputs": [],
   "source": [
    "max(len(comment) for comment in dataset_loving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9Z-B8RaCq7I"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "prompt_batch = remove_last_if_even(small_data)\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=80,\n",
    "    act_name=2,\n",
    "    prompt=not_toxic_comments,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "not_toxic_steered_df = gen_using_activation_additions(\n",
    "    prompt_batch=prompt_batch,\n",
    "    model=smol_model,\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    addition_location=\"front\",\n",
    "    seed=27,\n",
    "    **default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SNUUmv4Cq7I",
    "outputId": "bc84836c-9f91-4c21-d691-fdc958059b49"
   },
   "outputs": [],
   "source": [
    "print_latex_random_continuation(not_toxic_steered_df, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeyiKAe8Cq7I",
    "outputId": "cf479952-60bf-4949-9c0b-aae75ce3812a"
   },
   "outputs": [],
   "source": [
    "print_latex_random_continuation(loving_steered_df, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GxaY8tECq7I",
    "outputId": "68b54df9-19a1-4e51-ba46-f0ae8b059acb"
   },
   "outputs": [],
   "source": [
    "print_latex_random_continuation(unsteered_df, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u99xcqOTCq7I",
    "outputId": "e754bf4c-2498-4413-b86c-12a397316fbc"
   },
   "outputs": [],
   "source": [
    "print_latex_random_continuation(turner_steered_df_2, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uROftcO3Cq7I"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_latex_random_sample(ds, n=5, seed=0):\n",
    "    print(r'\\begin{itemize}')\n",
    "    random.seed(seed)\n",
    "    for sample in random.sample(ds, n):\n",
    "        print(r'\\item ' + sample)\n",
    "    print(r'\\end{itemize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2XucelKCq7I",
    "outputId": "4bb06caf-d704-440c-fa4c-848074cddabd"
   },
   "outputs": [],
   "source": [
    "print_latex_random_sample(not_toxic_comments, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_V9NuJi_Cq7I",
    "outputId": "3472d563-3ad3-4c2d-bf6d-304dd3d7fdc3"
   },
   "outputs": [],
   "source": [
    "print_latex_random_sample(dataset_loving, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7QNj1XQCq7I",
    "outputId": "c3cb9022-b383-4c57-d856-0610d39d3239"
   },
   "outputs": [],
   "source": [
    "print_latex_random_sample(filtered_toxic_ds['text'], n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U90FkSGcCq7J",
    "outputId": "bf938807-8f7b-4bc4-85c7-329f1712cd4b"
   },
   "outputs": [],
   "source": [
    "print_latex_random_sample(fantasy_ds, n=3, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vANx1r5LCq7J",
    "outputId": "95f5e113-7152-4fc0-881e-2baa039ee481"
   },
   "outputs": [],
   "source": [
    "print_latex_random_sample(scifi_ds, n=3, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlOTSneiCq7J",
    "outputId": "d60f90f3-5f84-401e-d793-13e56372662e"
   },
   "outputs": [],
   "source": [
    "print_latex_random_sample(sports_ds, n=3, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBFhVPiUCq7J"
   },
   "outputs": [],
   "source": [
    "df_sentiments_non_toxic_steered = compute_fragment_index_sentiments([(completion, 0) for completion in not_toxic_steered_df.completions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRYPq5dcCq7J",
    "outputId": "cefb9d62-bc6a-497f-9c2b-ddcf0bf63477"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams.update({\n",
    "    'font.family':'serif',\n",
    "    \"text.usetex\": False,\n",
    "    'savefig.facecolor': 'white',\n",
    "})\n",
    "\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "# plt.rc('xtick', labelsize=16)\n",
    "# plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=11)\n",
    "# plt.rc('figure', titlesize=20)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.lineplot(data=df_sentiments_steered, x='index', y='sentiment', label='Loving Steered', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_non_toxic_steered, x='index', y='sentiment', label='Non-Toxic Steered', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_turner_steered_2, x='index', y='sentiment', label='ActAdd (Turner et al.)', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_unsteered, x='index', y='sentiment', label='Unsteered', errorbar=('ci', 95))\n",
    "plt.xlabel('Words Generated After Prompt')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.xlim([0, 60])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('steered-vs-unsteered-toxic-comment-sentiments-shorter.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw7fObPXCq7J",
    "outputId": "b384ef0c-11ee-4f57-d590-7c5e6506b03a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams.update({\n",
    "    'font.family':'serif',\n",
    "    \"text.usetex\": False,\n",
    "    'savefig.facecolor': 'white',\n",
    "})\n",
    "\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "# plt.rc('xtick', labelsize=16)\n",
    "# plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=11)\n",
    "# plt.rc('figure', titlesize=20)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.lineplot(data=df_sentiments_steered, x='index', y='sentiment', label='Loving Steered', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_non_toxic_steered, x='index', y='sentiment', label='Non-Toxic Steered', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_turner_steered_2, x='index', y='sentiment', label='ActAdd (Turner et al.)', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_unsteered, x='index', y='sentiment', label='Unsteered', errorbar=('ci', 95))\n",
    "sns.lineplot(data=df_sentiments_turner_steered, x='index', y='sentiment', label='ActAdd (Turner et al.) ($\\lambda=5$)', errorbar=('ci', 95))\n",
    "plt.xlabel('Words Generated After Prompt')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.xlim([0, 60])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('steered-vs-unsteered-toxic-comment-sentiments-extra.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn8_wYpZCq7J",
    "outputId": "03da4968-7000-4b48-ed02-0ce14e120b85"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# load tokenizer and model weights\n",
    "tox_tokenizer = RobertaTokenizer.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier')\n",
    "toxicity_model = RobertaForSequenceClassification.from_pretrained('SkolkovoInstitute/roberta_toxicity_classifier').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot866a5XCq7J"
   },
   "outputs": [],
   "source": [
    "def compute_toxicity(inputs, batch_size=64):\n",
    "    if not isinstance(inputs, list):\n",
    "        inputs = [inputs]\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        tokens_batch = [tox_tokenizer.tokenize(inp) for inp in inputs[i:i+batch_size]]\n",
    "        ids_batch = [tox_tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_batch]\n",
    "        longest_len = max(map(len, ids_batch))\n",
    "        batch = torch.tensor([\n",
    "            [*([tox_tokenizer.pad_token_id] * (longest_len - len(inp))), *inp]\n",
    "            for inp in ids_batch\n",
    "        ]).cuda()\n",
    "        # inference\n",
    "        tox_outs = toxicity_model(batch)\n",
    "        probs = torch.softmax(tox_outs.logits, dim=1).detach()\n",
    "        outputs.append(probs[:, 1:2])\n",
    "\n",
    "    return torch.cat(outputs, dim=0).squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CWmS5awCq7J",
    "outputId": "5eb9d8be-13f5-4ad2-d5ce-d36b5b0bb0ec"
   },
   "outputs": [],
   "source": [
    "compute_toxicity(['hello friend']), compute_toxicity(['fuck you bastard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qchhOR-JCq7J",
    "outputId": "22aee0c8-ea91-45d8-d99d-7157b721d3ff"
   },
   "outputs": [],
   "source": [
    "compute_toxicity(['hello friend', 'fuck you bastard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wgr5NKb5Cq7J"
   },
   "outputs": [],
   "source": [
    "def process_sentence_toxicity_changes(sentences, start_indices=None):\n",
    "\n",
    "    sentence_fragments = get_sentence_fragments(sentences, start_indices)\n",
    "\n",
    "    toxicities = compute_toxicity([\n",
    "      ' '.join(fragment) for fragment in sentence_fragments\n",
    "    ])\n",
    "\n",
    "    return pd.DataFrame([\n",
    "      {'index': len(sentence_fragment), 'toxicity': toxicity}\n",
    "      for sentence_fragment, toxicity in zip(sentence_fragments, toxicities)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cHSbnn5Cq7J"
   },
   "outputs": [],
   "source": [
    "df_toxicity_non_toxic_steered = process_sentence_toxicity_changes(list(not_toxic_steered_df.completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rUYA68MCq7J"
   },
   "outputs": [],
   "source": [
    "non_toxic_steered_toxicities = compute_toxicity(list(not_toxic_steered_df.completions))\n",
    "loving_steered_toxicities = compute_toxicity(list(loving_steered_df.completions))\n",
    "unsteered_toxicities = compute_toxicity(list(unsteered_df.completions))\n",
    "turner_steered_2_toxicities = compute_toxicity(list(turner_steered_df_2.completions))\n",
    "# turner_steered_toxicities = compute_toxicity(list(turner_steered_df.completions))\n",
    "toxicities_df = pd.DataFrame(\n",
    "    [{'method': 'Loving Steered', 'toxicity': toxicity} for toxicity in loving_steered_toxicities] +\n",
    "    [{'method': 'ActAdd (Turner et al.)', 'toxicity': toxicity} for toxicity in turner_steered_2_toxicities] +\n",
    "    [{'method': 'Non-Toxic Steered', 'toxicity': toxicity} for toxicity in non_toxic_steered_toxicities] +\n",
    "    [{'method': 'Unsteered', 'toxicity': toxicity} for toxicity in unsteered_toxicities]\n",
    "    # + [{'method': 'ActAdd (Turner et al.) ($\\lambda=5$)', 'toxicity': toxicity} for toxicity in turner_steered_toxicities]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12guUH2hCq7J",
    "outputId": "ae913811-9ff1-468f-eedb-fe8def995f4b"
   },
   "outputs": [],
   "source": [
    "toxicities_df['log_tox'] = np.log(toxicities_df['toxicity'])\n",
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "# plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=14)\n",
    "# plt.rc('figure', titlesize=20)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.boxplot(toxicities_df, y='method', x='log_tox', orient='h')\n",
    "plt.xlabel('Toxicity Log-Probability')\n",
    "plt.ylabel('Generation Method')\n",
    "plt.savefig('toxicity-boxplot.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9UxtwRdCq7J"
   },
   "outputs": [],
   "source": [
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "prompt_batch = remove_last_if_even(small_data)\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsUdgHyQCq7J",
    "outputId": "00096ca9-4815-4478-c980-bb45abc9690c"
   },
   "outputs": [],
   "source": [
    "coefs_sweep = list(range(5, 125, 15))\n",
    "coefs_sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10y-JvSICq7J"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.prompt_utils import ActivationAddition, get_x_vector\n",
    "\n",
    "coef_toxicities_results = []\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for coef in coefs_sweep:\n",
    "    summand = [*get_x_vector(\n",
    "        prompt1='Love',\n",
    "        prompt2='Hate',\n",
    "        coeff=coef,\n",
    "        act_name=2,\n",
    "        model=smol_model,\n",
    "    )]\n",
    "    # summand: List[ActivationAddition] = [\n",
    "    #     *get_x_vector_preset(\n",
    "    #         prompt1=\"Love\",\n",
    "    #         prompt2=\" \",\n",
    "    #         coeff=3,\n",
    "    #         act_name=10,\n",
    "    #     )\n",
    "    # ]\n",
    "\n",
    "    turner_steered_df_coef = gen_using_activation_additions(\n",
    "        prompt_batch=prompt_batch,\n",
    "        model=smol_model,\n",
    "        activation_additions=summand,\n",
    "        addition_location=\"front\",\n",
    "        seed=27,\n",
    "        **default_kwargs\n",
    "    )\n",
    "\n",
    "    coef_toxicities_results.append((coef, turner_steered_df_coef,\n",
    "                                    compute_toxicity(list(turner_steered_df_coef.completions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G59y67qPCq7K",
    "outputId": "a28ab37b-bf28-4edb-efd0-4fe1f627d066"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "loving_coef_toxicities_results = []\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=0,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "for coef in coefs_sweep:\n",
    "    print(f'Computing for coef={coef}')\n",
    "    activation_addition_dataset[0].coeff = coef\n",
    "\n",
    "    loving_steered_df_coef = gen_using_activation_additions(\n",
    "        prompt_batch=prompt_batch,\n",
    "        model=smol_model,\n",
    "        activation_additions=activation_addition_dataset,\n",
    "        addition_location=\"front\",\n",
    "        seed=27,\n",
    "        **default_kwargs\n",
    "    )\n",
    "\n",
    "    loving_coef_toxicities_results.append((coef, loving_steered_df_coef,\n",
    "                                           compute_toxicity(list(loving_steered_df_coef.completions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3EHgjZNCq7K",
    "outputId": "5194d508-0328-48b1-9eb5-7ab5df54bea7"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "non_toxic_coef_toxicities_results = []\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=0,\n",
    "    act_name=2,\n",
    "    prompt=not_toxic_comments,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "for coef in coefs_sweep:\n",
    "    print(f'Computing for coef={coef}')\n",
    "    activation_addition_dataset[0].coeff = coef\n",
    "\n",
    "    non_toxic_steered_df_coef = gen_using_activation_additions(\n",
    "        prompt_batch=prompt_batch,\n",
    "        model=smol_model,\n",
    "        activation_additions=activation_addition_dataset,\n",
    "        addition_location=\"front\",\n",
    "        seed=27,\n",
    "        **default_kwargs\n",
    "    )\n",
    "\n",
    "    non_toxic_coef_toxicities_results.append((coef, non_toxic_steered_df_coef,\n",
    "                                              compute_toxicity(list(non_toxic_steered_df_coef.completions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8mpJX3WCq7K"
   },
   "outputs": [],
   "source": [
    "include_coefs = [5, 20, 50, 80, 110]\n",
    "coefs_toxicities_df = pd.DataFrame(\n",
    "    [{'method': f'ActAdd (Turner et al.) ($\\lambda={coef}$)', 'toxicity': t, 'coef': coef}\n",
    "     for coef, _,  toxicity in coef_toxicities_results if coef in include_coefs for t in toxicity]\n",
    "    + [{'method': f'Loving Steering ($\\lambda={coef}$)', 'toxicity': t, 'coef': coef}\n",
    "     for coef, _,  toxicity in loving_coef_toxicities_results if coef in include_coefs  for t in toxicity]\n",
    "    + [{'method': f'Non-Toxic Steering ($\\lambda={coef}$)', 'toxicity': t, 'coef': coef}\n",
    "     for coef, _,  toxicity in non_toxic_coef_toxicities_results if coef in include_coefs for t in toxicity]\n",
    "    # [{'method': 'Loving Steered', 'toxicity': toxicity} for toxicity in loving_steered_toxicities] +\n",
    "    # [{'method': 'Non-Toxic Steered', 'toxicity': toxicity} for toxicity in non_toxic_steered_toxicities] +\n",
    "    + [{'method': 'Unsteered', 'toxicity': toxicity, 'coef': 0} for toxicity in compute_toxicity(list(unsteered_df.completions))]\n",
    ")\n",
    "coefs_toxicities_df['log_tox'] = np.log(coefs_toxicities_df['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52_S3wCdCq7K",
    "outputId": "26a1f74a-3cdd-4bed-a8a6-4bf09187578e"
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=16)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=14)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.boxplot(coefs_toxicities_df, y='method', x='log_tox', orient='h')\n",
    "plt.xlabel('Toxicity Log-Probability')\n",
    "plt.ylabel('Generation Method')\n",
    "plt.savefig('toxicity-boxplot-act-add-coefs.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-x9UNa1LCq7K",
    "outputId": "3987f1e0-03fa-4d4a-acc5-ecc4a7f8d892"
   },
   "outputs": [],
   "source": [
    "coefs_toxicities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80Vdo0EcCq7K",
    "outputId": "0118c1e4-7eb1-4abe-b83c-05d61ca4c181"
   },
   "outputs": [],
   "source": [
    "coefs_toxicities_df_not_grouped = pd.DataFrame(\n",
    "    [{'method': f'ActAdd (Turner et al.)', 'toxicity': t, 'coef': coef}\n",
    "     for coef, _,  toxicity in coef_toxicities_results for t in toxicity]\n",
    "    + [{'method': f'Loving Steering', 'toxicity': t, 'coef': coef}\n",
    "     for coef, _,  toxicity in loving_coef_toxicities_results for t in toxicity]\n",
    "    + [{'method': f'Non-Toxic Steering', 'toxicity': t, 'coef': coef}\n",
    "     for coef, _,  toxicity in non_toxic_coef_toxicities_results for t in toxicity]\n",
    "    # [{'method': 'Loving Steered', 'toxicity': toxicity} for toxicity in loving_steered_toxicities] +\n",
    "    # [{'method': 'Non-Toxic Steered', 'toxicity': toxicity} for toxicity in non_toxic_steered_toxicities] +\n",
    "    # + [{'method': 'Unsteered', 'toxicity': toxicity, 'coef': 0} for toxicity in compute_toxicity(list(unsteered_df.completions))]\n",
    ")\n",
    "coefs_toxicities_df_not_grouped['log_tox'] = np.log(coefs_toxicities_df_not_grouped['toxicity'])\n",
    "sns.lineplot(data=coefs_toxicities_df_not_grouped, x='coef', y='log_tox', hue='method')\n",
    "means_df = coefs_toxicities_df_not_grouped.groupby(['method', 'coef']).mean().reset_index()\n",
    "sns.scatterplot(data=means_df, x='coef', y='log_tox', hue='method', legend=False)\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks(coefs_sweep)\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel('Steering Coefficient')\n",
    "plt.ylabel('Toxicity Log-Probability')\n",
    "plt.savefig('toxicity-lineplot-coefs-sweep.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-yHtZDVCq7K",
    "outputId": "fa131a2e-f283-4978-bff7-00aa2d02ba2b"
   },
   "outputs": [],
   "source": [
    "turner_steered_df_coef10 = coef_toxicities_results[1][1]\n",
    "print_latex_random_continuation(turner_steered_df_coef10, n=5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGuT-sg9Cq7K",
    "outputId": "49ca92a1-2e97-4021-82ac-b155401adfd2"
   },
   "outputs": [],
   "source": [
    "list(not_toxic_steered_df.completions)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naQCuTIFCq7K"
   },
   "outputs": [],
   "source": [
    "df_toxicity_loving_steered = process_sentence_toxicity_changes(list(loving_steered_df.completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f41tNC2MCq7K",
    "outputId": "ab311f5d-2196-43bf-cd84-890298e4ffc6"
   },
   "outputs": [],
   "source": [
    "list(loving_steered_df.completions)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdAFhJfSCq7K",
    "outputId": "366ee151-0242-4226-e196-a9388326700c"
   },
   "outputs": [],
   "source": [
    "compute_toxicity([' '.join(completion.split()[:4]) for completion in list(loving_steered_df.completions)[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8jqcNRmCq7K"
   },
   "outputs": [],
   "source": [
    "df_toxicity_unsteered = process_sentence_toxicity_changes(list(unsteered_df.completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVdbhAngCq7K",
    "outputId": "72ee1b3e-4065-4489-d7bc-35b75d62cb0d"
   },
   "outputs": [],
   "source": [
    "list(unsteered_df.completions)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgWg1orLCq7K",
    "outputId": "82d1f958-a5ed-4d54-ecb9-13127ed198dd"
   },
   "outputs": [],
   "source": [
    "df_toxicity_non_toxic_steered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1MW_VJfCq7K",
    "outputId": "c501649f-51af-4f24-e17e-a42d49da2c4b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams.update({\n",
    "    'font.family':'serif',\n",
    "    \"text.usetex\": False,\n",
    "    'savefig.facecolor': 'white',\n",
    "})\n",
    "\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('axes', titlesize=16)\n",
    "plt.rc('axes', labelsize=16)\n",
    "# plt.rc('xtick', labelsize=16)\n",
    "# plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=18)\n",
    "# plt.rc('figure', titlesize=20)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.lineplot(data=df_toxicity_loving_steered, x='index', y='toxicity', label='Loving Steered')\n",
    "sns.lineplot(data=df_toxicity_non_toxic_steered, x='index', y='toxicity', label='Non-Toxic Steered')\n",
    "sns.lineplot(data=df_toxicity_unsteered, x='index', y='toxicity', label='Unsteered')\n",
    "plt.xlabel('Words Generated After Prompt')\n",
    "plt.ylabel('Average Toxicity')\n",
    "plt.xlim([0, 60])\n",
    "plt.savefig('steered-vs-unsteered-toxic-comment-sentiments.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3t0hhqtpF67"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or2e89Z5HNzf"
   },
   "source": [
    "# Experiment with both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cp_2F6qvHPeH",
    "outputId": "a13cb9cd-5e8c-4e17-9078-f0e8a2803647"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "\n",
    "steering_sentiment_experiment(\n",
    "  input_dataset=input_dataset,\n",
    "  n_completions=6,\n",
    "  model=smol_model,\n",
    "  activation_addition_dataset=activation_addition_dataset,\n",
    "  addition_location=\"front\",\n",
    "  seed=21,\n",
    "  default_kwargs = default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y0JN6yYEX1Tr",
    "outputId": "3a206647-c0b7-4341-dcc7-be1884fe18d1"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "\n",
    "steering_sentiment_experiment(\n",
    "  input_dataset=input_dataset,\n",
    "  n_completions=6,\n",
    "  model=smol_model,\n",
    "  activation_addition_dataset=activation_addition_dataset,\n",
    "  addition_location=\"front\",\n",
    "  seed=21,\n",
    "  default_kwargs = default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iwWW6blMkEJa",
    "outputId": "f7b6b20b-9b9d-46ca-cf5a-7b9bfc9597d7"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "\n",
    "steering_sentiment_experiment(\n",
    "  input_dataset=input_dataset,\n",
    "  n_completions=6,\n",
    "  model=smol_model,\n",
    "  activation_addition_dataset=activation_addition_dataset,\n",
    "  addition_location=\"front\",\n",
    "  seed=21,\n",
    "  default_kwargs = default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WclCpkDkWKub",
    "outputId": "31c34ffd-5fc0-44ad-97af-de1d20e8476f"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "\n",
    "steering_sentiment_experiment(\n",
    "  input_dataset=input_dataset,\n",
    "  n_completions=6,\n",
    "  model=smol_model,\n",
    "  activation_addition_dataset=activation_addition_dataset,\n",
    "  addition_location=\"back\",\n",
    "  seed=21,\n",
    "  default_kwargs = default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dY3TSWYvQ1EJ",
    "outputId": "35bbffce-9108-4b0b-c1a8-78b589f54181"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "\n",
    "steering_sentiment_experiment(\n",
    "  input_dataset=input_dataset,\n",
    "  n_completions=20,\n",
    "  model=smol_model,\n",
    "  activation_addition_dataset=activation_addition_dataset,\n",
    "  addition_location=\"front\",\n",
    "  seed=21,\n",
    "  default_kwargs = default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8IV0sp8Qgig"
   },
   "source": [
    "#Repeating for GPT-2 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94imDEIbQgOt"
   },
   "outputs": [],
   "source": [
    "from algebraic_value_editing.dataset_utils import ActivationAdditionDataset\n",
    "\n",
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "default_kwargs = {'temperature': 1, 'freq_penalty': 1, 'top_p': .3, 'tokens_to_generate' : 80}\n",
    "\n",
    "input_dataset = filtered_toxic_ds[0:100]['text']\n",
    "\n",
    "steering_sentiment_experiment(\n",
    "  input_dataset=input_dataset,\n",
    "  n_completions=6,\n",
    "  model=smol_model,\n",
    "  activation_addition_dataset=activation_addition_dataset,\n",
    "  addition_location=\"front\",\n",
    "  seed=21,\n",
    "  default_kwargs = default_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bGYOQBrEcJl"
   },
   "source": [
    "# Trying random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVItMA1iYZU7"
   },
   "outputs": [],
   "source": [
    "halfway_data = [first_half_string(s) for s in input_dataset]\n",
    "small_data = [s for s in halfway_data if len(smol_model.tokenizer(s)[\"input_ids\"]) < 200]\n",
    "\n",
    "small_data = remove_last_if_even(small_data)\n",
    "\n",
    "prompt_batch = small_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-Q04rruEb7M"
   },
   "outputs": [],
   "source": [
    "result = gen_using_model(\n",
    "    model= smol_model,\n",
    "    prompt_batch = prompt_batch[0],\n",
    "    seed = 0,\n",
    "    include_logits = False,\n",
    "    log = False,  # pylint: disable=unused-argument\n",
    "    **default_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "d5I9k7mpYwWS",
    "outputId": "bf1a753f-92a4-404b-f7c2-7234f268e1c7"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "c6Bi8GUP493D",
    "outputId": "07ae0ce4-4553-4214-dece-75031d8d9b5d"
   },
   "outputs": [],
   "source": [
    "result[\"prompts\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "KOWEtDScYzt3",
    "outputId": "fe4b0dbc-6e28-46b4-cbc3-c8a34e3da414"
   },
   "outputs": [],
   "source": [
    "result['completions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZDEYXYIY-Xn",
    "outputId": "c919cd62-eb07-4aef-9e71-4dba4933eb24"
   },
   "outputs": [],
   "source": [
    "activation_addition_dataset = [ActivationAdditionDataset(\n",
    "    coeff=60,\n",
    "    act_name=2,\n",
    "    prompt=dataset_loving,\n",
    "    from_dataset=True,\n",
    "    use_all_activations=True,\n",
    "    prompt_2=tiny_training_subset[:200],\n",
    "    from_pca=False,\n",
    "    from_difference=True,\n",
    ")]\n",
    "\n",
    "\n",
    "steered_result = gen_using_activation_additions(\n",
    "    model= smol_model,\n",
    "    prompt_batch = prompt_batch[0],\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    seed = 0,\n",
    "    include_logits = False,\n",
    "    log = False,  # pylint: disable=unused-argument\n",
    "    **default_kwargs,\n",
    ")\n",
    "\n",
    "# gen_using_activation_additions(\n",
    "    # prompt_batch = [\"I hate you because\"] * 8,\n",
    "    # model=model,\n",
    "    # activation_additions=activation_addition_dataset,\n",
    "    # addition_location=\"front\",\n",
    "    # seed=0,\n",
    "    # **default_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "bPff2aK3aLBI",
    "outputId": "5aabe095-8a11-42e0-cb2b-359ffa6a0b16"
   },
   "outputs": [],
   "source": [
    "steered_result['prompts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "xIF0ticRZzkK",
    "outputId": "6e4cc51b-0102-4746-8d1a-626ad30cfc21"
   },
   "outputs": [],
   "source": [
    "steered_result['completions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIK0f1liarrH"
   },
   "outputs": [],
   "source": [
    "steered_result_2 = gen_using_activation_additions(\n",
    "    model= smol_model,\n",
    "    prompt_batch = prompt_batch[0],\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    seed = 0,\n",
    "    include_logits = False,\n",
    "    log = False,  # pylint: disable=unused-argument\n",
    "    **default_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TOlV96NauTX"
   },
   "outputs": [],
   "source": [
    "def sample(seed, index):\n",
    "  steered_result = gen_using_activation_additions(\n",
    "    model= smol_model,\n",
    "    prompt_batch = prompt_batch[index],\n",
    "    activation_additions=activation_addition_dataset,\n",
    "    seed = seed,\n",
    "    include_logits = False,\n",
    "    log = False,  # pylint: disable=unused-argument\n",
    "    **default_kwargs,\n",
    "  )\n",
    "  unsteered_result = gen_using_model(\n",
    "    model= smol_model,\n",
    "    prompt_batch = prompt_batch[index],\n",
    "    seed = seed,\n",
    "    include_logits = False,\n",
    "    log = False,  # pylint: disable=unused-argument\n",
    "    **default_kwargs,\n",
    "  )\n",
    "\n",
    "  return steered_result['prompts'][0] + steered_result['completions'][0], unsteered_result['prompts'][0] + unsteered_result['completions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-tlPvfZa6hd",
    "outputId": "5acc6de6-af6f-450d-beb8-0d0cd8d7918a"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for seed in range(3):\n",
    "  results[seed] = {}\n",
    "  for index in range(5):\n",
    "    results[seed][index] = sample(seed, index)\n",
    "\n",
    "for seed in range(3):\n",
    "  for index in range(5):\n",
    "    print(\"steered response: \" + results[seed][index][0])\n",
    "    print(\"unsteered response: \"+ results[seed][index][1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
