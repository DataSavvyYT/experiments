{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DataSavvyYT/experiments/blob/main/llm_steering/hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "430zhOo5Powm"
   },
   "outputs": [],
   "source": [
    "# @title 1. Install Dependencies\n",
    "# Run this cell first to install the necessary libraries.\n",
    "!pip install -q transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHSCVd-NqFQY"
   },
   "outputs": [],
   "source": [
    "# @title 2. Load Model and Define Steering Functions\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2KJJF-dGjfl"
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_name = \"gpt2-medium\"  # You can swap this for 'gpt2-xl' or 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "layer_id = 15               # The layer to inject the steering vector (0-23 for gpt2-medium)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {model_name} on {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_TtzdZqGmPj"
   },
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def get_activations(text):\n",
    "    \"\"\"\n",
    "    Runs the model and extracts the hidden states from the specified layer\n",
    "    for the last token in the sequence.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Hidden states tuple: (layer_0, layer_1, ..., layer_N)\n",
    "    # We grab the specific layer we want to steer.\n",
    "    # Shape: [batch, sequence_length, hidden_dim]\n",
    "    hidden_state = outputs.hidden_states[layer_id]\n",
    "\n",
    "    # We return the activation of the LAST token.\n",
    "    # This represents the \"summary\" of the concept at this point.\n",
    "    return hidden_state[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3nekbFdGr90"
   },
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, steering_vector, multiplier=1.0, max_new_tokens=30):\n",
    "    \"\"\"\n",
    "    Generates text while injecting the steering vector into the forward pass.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Define the hook function\n",
    "    def steering_hook(module, input, output):\n",
    "        # Output of a layer in HF is usually (hidden_states,)\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "\n",
    "        # Add the steering vector to all token positions\n",
    "        # The vector shape is [hidden_dim], we broadcast it to [batch, seq_len, hidden_dim]\n",
    "        hidden_states += (steering_vector * multiplier)\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            return (hidden_states,) + output[1:]\n",
    "        return hidden_states\n",
    "\n",
    "    # Register the hook on the specific layer\n",
    "    # For GPT-2, layers are in model.transformer.h\n",
    "    # For Llama, it might be model.model.layers\n",
    "    try:\n",
    "        layer_module = model.transformer.h[layer_id]\n",
    "    except AttributeError:\n",
    "        # Fallback for Llama/Mistral architectures\n",
    "        layer_module = model.model.layers[layer_id]\n",
    "\n",
    "    handle = layer_module.register_forward_hook(steering_hook)\n",
    "\n",
    "    # Generate text\n",
    "    try:\n",
    "        output_sequences = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    finally:\n",
    "        # VERY IMPORTANT: Remove the hook so the model goes back to normal\n",
    "        handle.remove()\n",
    "\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Setup complete. Ready to steer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-gghbeFGzd3"
   },
   "outputs": [],
   "source": [
    "# @title 3. Create the Steering Vector\n",
    "# We create a vector that represents the direction \"Anger\" vs \"Calmness\".\n",
    "\n",
    "# 1. Define contrasting prompts\n",
    "# Ideally, these prompts should be identical except for the concept you want to isolate.\n",
    "pos_concept = \"I am feeling extremely happy, loving, and peaceful today.\"\n",
    "neg_concept = \"I am feeling extremely angry, hateful, and furious today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsjph3a8G2Um"
   },
   "outputs": [],
   "source": [
    "# 2. Extract activations\n",
    "print(f\"Extracting vector: '{pos_concept}' minus '{neg_concept}'\")\n",
    "pos_act = get_activations(pos_concept)\n",
    "neg_act = get_activations(neg_concept)\n",
    "\n",
    "# 3. Create the steering vector (Positive - Negative)\n",
    "# This vector points in the \"direction\" of Happiness/Peace.\n",
    "steering_vec = pos_act - neg_act\n",
    "\n",
    "print(\"Steering vector created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_nGE2IZG5wc"
   },
   "outputs": [],
   "source": [
    "# @title 4. Test the Steering\n",
    "# Now we test it on a neutral prompt to see if we can force the mood.\n",
    "\n",
    "test_prompt = \"I went to the store and the clerk said\"\n",
    "\n",
    "print(f\"--- BASELINE (Multiplier: 0.0) ---\")\n",
    "print(generate_with_steering(test_prompt, steering_vec, multiplier=0.0))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"--- STEERED POSITIVE (Happy/Peaceful) (Multiplier: 3.0) ---\")\n",
    "print(generate_with_steering(test_prompt, steering_vec, multiplier=3.0))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"--- STEERED NEGATIVE (Angry/Hateful) (Multiplier: -3.0) ---\")\n",
    "# Using a negative multiplier pushes the model in the opposite direction (towards the negative concept)\n",
    "print(generate_with_steering(test_prompt, steering_vec, multiplier=-3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUeTG3EYG9on"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
