{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataSavvyYT/experiments/blob/main/llm_steering/hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Install Dependencies\n",
        "# Run this cell first to install the necessary libraries.\n",
        "!pip install -q transformers torch accelerate"
      ],
      "metadata": {
        "id": "430zhOo5Powm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load Model and Define Steering Functions\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "tHSCVd-NqFQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "model_name = \"gpt2-medium\"  # You can swap this for 'gpt2-xl' or 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "layer_id = 15               # The layer to inject the steering vector (0-23 for gpt2-medium)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Loading {model_name} on {device}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "X2KJJF-dGjfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def get_activations(text):\n",
        "    \"\"\"\n",
        "    Runs the model and extracts the hidden states from the specified layer\n",
        "    for the last token in the sequence.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    # Hidden states tuple: (layer_0, layer_1, ..., layer_N)\n",
        "    # We grab the specific layer we want to steer.\n",
        "    # Shape: [batch, sequence_length, hidden_dim]\n",
        "    hidden_state = outputs.hidden_states[layer_id]\n",
        "\n",
        "    # We return the activation of the LAST token.\n",
        "    # This represents the \"summary\" of the concept at this point.\n",
        "    return hidden_state[0, -1, :]"
      ],
      "metadata": {
        "id": "s_TtzdZqGmPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_steering(prompt, steering_vector, multiplier=1.0, max_new_tokens=30):\n",
        "    \"\"\"\n",
        "    Generates text while injecting the steering vector into the forward pass.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Define the hook function\n",
        "    def steering_hook(module, input, output):\n",
        "        # Output of a layer in HF is usually (hidden_states,)\n",
        "        if isinstance(output, tuple):\n",
        "            hidden_states = output[0]\n",
        "        else:\n",
        "            hidden_states = output\n",
        "\n",
        "        # Add the steering vector to all token positions\n",
        "        # The vector shape is [hidden_dim], we broadcast it to [batch, seq_len, hidden_dim]\n",
        "        hidden_states += (steering_vector * multiplier)\n",
        "\n",
        "        if isinstance(output, tuple):\n",
        "            return (hidden_states,) + output[1:]\n",
        "        return hidden_states\n",
        "\n",
        "    # Register the hook on the specific layer\n",
        "    # For GPT-2, layers are in model.transformer.h\n",
        "    # For Llama, it might be model.model.layers\n",
        "    try:\n",
        "        layer_module = model.transformer.h[layer_id]\n",
        "    except AttributeError:\n",
        "        # Fallback for Llama/Mistral architectures\n",
        "        layer_module = model.model.layers[layer_id]\n",
        "\n",
        "    handle = layer_module.register_forward_hook(steering_hook)\n",
        "\n",
        "    # Generate text\n",
        "    try:\n",
        "        output_sequences = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    finally:\n",
        "        # VERY IMPORTANT: Remove the hook so the model goes back to normal\n",
        "        handle.remove()\n",
        "\n",
        "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Setup complete. Ready to steer!\")"
      ],
      "metadata": {
        "id": "O3nekbFdGr90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Create the Steering Vector\n",
        "# We create a vector that represents the direction \"Anger\" vs \"Calmness\".\n",
        "\n",
        "# 1. Define contrasting prompts\n",
        "# Ideally, these prompts should be identical except for the concept you want to isolate.\n",
        "pos_concept = \"I am feeling extremely happy, loving, and peaceful today.\"\n",
        "neg_concept = \"I am feeling extremely angry, hateful, and furious today.\""
      ],
      "metadata": {
        "id": "C-gghbeFGzd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Extract activations\n",
        "print(f\"Extracting vector: '{pos_concept}' minus '{neg_concept}'\")\n",
        "pos_act = get_activations(pos_concept)\n",
        "neg_act = get_activations(neg_concept)\n",
        "\n",
        "# 3. Create the steering vector (Positive - Negative)\n",
        "# This vector points in the \"direction\" of Happiness/Peace.\n",
        "steering_vec = pos_act - neg_act\n",
        "\n",
        "print(\"Steering vector created.\")"
      ],
      "metadata": {
        "id": "zsjph3a8G2Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Test the Steering\n",
        "# Now we test it on a neutral prompt to see if we can force the mood.\n",
        "\n",
        "test_prompt = \"I went to the store and the clerk said\"\n",
        "\n",
        "print(f\"--- BASELINE (Multiplier: 0.0) ---\")\n",
        "print(generate_with_steering(test_prompt, steering_vec, multiplier=0.0))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"--- STEERED POSITIVE (Happy/Peaceful) (Multiplier: 3.0) ---\")\n",
        "print(generate_with_steering(test_prompt, steering_vec, multiplier=3.0))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"--- STEERED NEGATIVE (Angry/Hateful) (Multiplier: -3.0) ---\")\n",
        "# Using a negative multiplier pushes the model in the opposite direction (towards the negative concept)\n",
        "print(generate_with_steering(test_prompt, steering_vec, multiplier=-3.0))"
      ],
      "metadata": {
        "id": "j_nGE2IZG5wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PUeTG3EYG9on"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}