{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqM-T1RTzY6C"
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dUKJPsGDPQlS"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBKhPThKPac5"
   },
   "source": [
    "# Fine-tuning Gemma using Unsloth\n",
    "\n",
    "Welcome to this step-by-step guide on fine-tuning the [Gemma](https://huggingface.co/google/gemma-2b) using [Unsloth](https://unsloth.ai/).\n",
    "\n",
    "\n",
    "[**Gemma**](https://ai.google.dev/gemma) is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "[**Unsloth**](https://unsloth.ai/) is a Python framework developed for finetuning large language models like Gemma 2.4x faster, using 58% less VRAM, and with no degradation in accuracy.\n",
    "\n",
    "\n",
    "In this notebook, you will learn how to finetune Gemma 2 models using **Unsloth** in a Google Colab environment. You'll install the necessary packages, finetune the model, and run a sample inference.\n",
    "\n",
    "<table align=\"left\">\n",
    "<td>\n",
    " <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_2]Finetune_with_Unsloth.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j8QtM7uTQNd"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Select the Colab runtime\n",
    "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model. In this case, you can use a T4 GPU:\n",
    "\n",
    "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
    "2. Select **Change runtime type**.\n",
    "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
    "\n",
    "### Hugging Face setup\n",
    "\n",
    "**Before you dive into the tutorial, let's get you set up with Hugging Face. This is needed to upload the finetuned model into Hugging Face Hub.**\n",
    "\n",
    "1. **Hugging Face Account:**  If you don't already have one, you can create a free Hugging Face account by clicking [here](https://huggingface.co/join).\n",
    "2. **Hugging Face Token:**  Generate a Hugging Face access (with `write` permission) token by clicking [here](https://huggingface.co/settings/tokens). You'll need this token later in the tutorial.\n",
    "\n",
    "**Once you've completed these steps, you're ready to move on to the next section where you'll set up environment variables in your Colab environment.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMi-K8lVMdpE"
   },
   "source": [
    "### Configure your HF token\n",
    "\n",
    "Add your Hugging Face token to the Colab Secrets manager to securely store it.\n",
    "\n",
    "1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
    "2. Create a new secret with the name `HF_TOKEN`.\n",
    "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
    "4. Toggle the button on the left to allow notebook access to the secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYAHbvbeMhIF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
    "# vars as appropriate for your system.\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvC30lMdTZ4p"
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "You'll need to install a few Python packages and dependencies for Unsloth.\n",
    "\n",
    "Run the following cell to install or upgrade the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "2eSvM9zX_2d3",
    "outputId": "edc37fab-fd19-4d9a-b470-48267188a6e7"
   },
   "outputs": [],
   "source": [
    "#   Install Unsloth library\n",
    "! pip install -q unsloth --upgrade --no-cache-dir\n",
    "\n",
    "# Install Flash Attention 2 for softcapping support\n",
    "import torch\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install --no-deps -q packaging ninja einops \"flash-attn>=2.6.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "## Finetuning Gemma 2 using Unsloth library\n",
    "\n",
    "### Initializing Gemma 2 model\n",
    "\n",
    "Unsloth library supports a variety of open-source LLMs including Gemma. For this notebook, you will use Gemma 2's 2b model. You can load the Gemma 2 model in Unsloth using the `FastLanguageModel` class. To know more about the other variants of Gemma 2 model provided by Unsloth, visit the [Unsloth's supported models documentation](https://docs.unsloth.ai/get-started/all-our-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "82305d44082845d5a3cab38968e10ee7",
      "e13d6a3234c34c6894a9b3f08f7401a0",
      "18337511819f44bc9c9a505ce2945453",
      "b474787293b147e6be5c7e898e82f5f2",
      "035fd37b97c942b9964b5d868f80cd49",
      "df7113d85a724baba5be3d1e045d16e8"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "46af1b1c-145a-481c-8ca9-c7a709c758af"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,        # None for auto detection.\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvCgS45kZFdu"
   },
   "source": [
    "### Load a dataset\n",
    "\n",
    "For this guide, you'll use an existing dataset from Hugging Face. You can replace it with your own dataset if you prefer.\n",
    "\n",
    "The dataset chosen for this guide is [**yahma/alpaca-cleaned**](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a clean version of the original Alpaca dataset by Stanford. The Alpaca dataset is a collection of over 50,000 instructions and demonstrations that can be used to fine-tune language models to better understand and follow instructions.\n",
    "\n",
    "**Credits:** **https://huggingface.co/yahma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "67031f495ee54b22b452e3e16f489335",
      "1d85ba466f6b465d8e77dee2742addea",
      "41940dec1dab42a78f086efcde3c7d83"
     ]
    },
    "id": "FTEOSyqzZ5wx",
    "outputId": "d316283a-9e08-4168-e4df-ac0e7a0079f2"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE5WGhfdcp_W"
   },
   "source": [
    "Let's look at a few samples to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGFAfMpZcooc",
    "outputId": "f75053f9-8987-4991-ef49-db2cbb4de33e"
   },
   "outputs": [],
   "source": [
    "dataset[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BEG9f4XdA3o",
    "outputId": "5d4ca5d4-8910-4fb3-d026-0c0aa07d92a3"
   },
   "outputs": [],
   "source": [
    "from google.colab import data_table\n",
    "import pandas as pd\n",
    "\n",
    "# Enable interactive DataFrame display\n",
    "data_table.enable_dataframe_formatter()\n",
    "\n",
    "# Convert the 'train' split to a Pandas DataFrame\n",
    "df_train = pd.DataFrame(dataset)\n",
    "\n",
    "# Select the first 5 rows\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAGUvIjReACW"
   },
   "source": [
    "### Set the prompt template\n",
    "\n",
    "Here you will define the Alpaca prompt template. This template has 3 sections:\n",
    "- Instruction\n",
    "- Input\n",
    "- Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an\n",
    "input that provides further context. Write a response that appropriately\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # EOS_TOKEN is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlLb28SSef_M"
   },
   "source": [
    "### Define the formatting function\n",
    "\n",
    "The formatting function applies the template created above to each row in the dataset and converts it into a format suited for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "cef957d88bd9482db3f624c69b747d69"
     ]
    },
    "id": "1UYqbBh5ed_e",
    "outputId": "f13d2443-4566-4bb3-ebfb-c99b5f550779"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    # EOS_TOKEN is necessary, otherwise your generation will go on forever!\n",
    "    texts = [alpaca_prompt_template.format(instruction, input, output) + EOS_TOKEN\n",
    "                                  for instruction, input, output in\n",
    "                                  zip(instructions, inputs, outputs)]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "### Set LoRA configuration\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows for efficient fine-tuning by adapting only a subset of model parameters.\n",
    "\n",
    "Here, you set the following parameters:\n",
    "- `r` to 16, which controls the rank of the adaptation matrices.\n",
    "- `lora_alpha` to 16 for scaling.\n",
    "- `lora_dropout` to 0 since it is optimized.\n",
    "\n",
    "To know more about LoRA parameters and their effects, check out the [LoRA parameters encyclopedia](https://github.com/unslothai/unsloth/wiki#lora-parameters-encyclopedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bZsfBuZDeCL",
    "outputId": "97811386-4b8d-45b0-a05b-0f9731e627e6"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA attention dimension\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,  # Alpha parameter for LoRA scaling\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # Rank stabilized LoRA\n",
    "    loftq_config = None, # LoRA-Fine-Tuning-Aware Quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "### Set training configuration\n",
    "\n",
    "Set up the training arguments that define how the model will be trained.\n",
    "\n",
    "Here, you'll define the following parameters:\n",
    "\n",
    "- For training and evaluation:\n",
    "  - `output directory`\n",
    "  - `max steps`\n",
    "  - `batch sizes`\n",
    "\n",
    "- To optimize the training process:\n",
    "  - `learning rate`\n",
    "  - `optimizer`\n",
    "  - `learning rate scheduler`\n",
    "\n",
    "**Note:** `max_steps` is set as 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i__N744Jgz5X"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 42,\n",
    "    output_dir = \"outputs\",\n",
    "    report_to = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "[Huggingface's TRL](https://huggingface.co/docs/trl/index) offers a user-friendly API for building SFT models and training them on your dataset with just a few lines of code. Here you will use Huggingface TRL's `SFTTrainer` class to train the model. This class inherits from the `Trainer` class available in the Transformers library, but is specifically optimized for supervised fine-tuning (instruction tuning). Read more about SFFTrainer from the [official TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "cb9944ca20b34cf6a7f33f8ef38142bc"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "453f56a9-48c4-4fa4-d65f-97c621329c68"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    # Setting packing as False can speed up training five times\n",
    "    # for short sequences.\n",
    "    packing = False,\n",
    "    args = training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10U7GGxLkee0"
   },
   "source": [
    "Now, let's start the fine-tuning process by calling `trainer.train()`, which uses `SFTTrainer` to handle the training loop, including data loading, forward and backward passes, and optimizer steps, all configured according to the settings you've provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqxqAZ7KJ4oL",
    "outputId": "c0b7635f-596c-4f2c-f95d-cebe47d78e88"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_SJ-iTxlH-c"
   },
   "source": [
    "### Save the model locally\n",
    "\n",
    "After training is complete, save the fine-tuned model by calling `save_pretrained(new_model)`. This saves the model weights and configuration files to the directory specified by `new_model` (**gemma_ft_unsloth**). You can reload and use the fine-tuned model later for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuLpAtt7lQdU",
    "outputId": "cfbb5e97-99c7-44c8-eabe-29c6549c3961"
   },
   "outputs": [],
   "source": [
    "new_model = \"gemma_ft_unsloth\"\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz41b3rDNegs"
   },
   "source": [
    "### Push to Hugging Face Hub\n",
    "\n",
    "You can use the model's `push_to_hub` method to upload the model to Hugging Face Hub.\n",
    "\n",
    "**Note**: In the following code snippets, replace \"your_hf_username\" to your Hugging Face username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4d-nreYNbYb"
   },
   "outputs": [],
   "source": [
    "# Push the trained model to Hub\n",
    "model.push_to_hub(\"harjeetkr/gemma_ft_unsloth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqQFyg00XyjO"
   },
   "source": [
    "If you only want to save the Lora adapters, uncomment and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fB6XaUbYJFo"
   },
   "outputs": [],
   "source": [
    "#model.push_to_hub_merged(\"your_hf_username/gemma_ft_lora\", tokenizer,\n",
    "#                         save_method = \"lora\", token = os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd_V_x6fYN8Y"
   },
   "source": [
    "If you want to save the model as GGUF, uncomment and run the code below. To check the different quantization methods supported by Unsloth, visit\n",
    "[Unsloth wiki's Save as GGUF section](https://github.com/unslothai/unsloth/wiki#saving-to-gguf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzU1I78wYts_"
   },
   "outputs": [],
   "source": [
    "#model.push_to_hub_gguf(\"your_hf_username/gemma_ft_q4_k_m\",\n",
    "#                       tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfyWrHUYY3AP"
   },
   "source": [
    "To know more about the different formats available in Unsloth, check out [Unsloth wiki's saving models section](https://github.com/unslothai/unsloth/wiki#saving-models-to-16bit-for-vllm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM-QLe32qzlF"
   },
   "source": [
    "## Inference\n",
    "\n",
    "### Prompt using the newly fine-tuned model\n",
    "\n",
    "\n",
    "Now that you've finally fine-tuned your custom Gemma model, let's reload the LoRA adapter weights and tokenizer to finally prompt using it and also verify if it's really working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzKkxC7arjxP",
    "outputId": "d57eb91c-0066-44cf-97c5-5e70eab8d470"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = new_model, # Your finetuned model name\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FU-E2kHrfo8"
   },
   "source": [
    "Now, test the fine-tuned model with a sample prompt by first using the tokenizer to generate the input ids, and then relying on the reloaded fine-tuned model to generate a response using `model.generate()`.\n",
    "\n",
    "Instead of waiting the entire time, you can view the generation token by token by using a `TextStreamer` for continuous inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQybHt5YnG9W",
    "outputId": "580e3e5e-9b3a-4b79-e896-edc475027c4d"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt_template.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k7OvHwLtQz0"
   },
   "source": [
    "Congratulations! You've successfully fine-tuned Gemma using Unsloth. You've covered the entire process, from setting up the environment to training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbJAeaK0tqMo"
   },
   "source": [
    "## What's next?\n",
    "Your next steps could include the following:\n",
    "\n",
    "- **Experiment with Different Datasets**: Try fine-tuning on other datasets in [Hugging Face](https://huggingface.co/docs/datasets/en/index) or your own data to adapt the model to various tasks or domains.\n",
    "\n",
    "- **Tune Hyperparameters**: Adjust training parameters (e.g., learning rate, batch size, epochs, LoRA settings) to optimize performance and improve training efficiency.\n",
    "\n",
    "- **Save model in different formats**: Check the different saving formats provided by Unsloth and use it in `llama.cpp` or a UI based system like `GPT4All`.\n",
    "\n",
    "By exploring these activities, you'll deepen your understanding and further enhance your fine-tuned Gemma model. Happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "name": "[Gemma_2]Finetune_with_Unsloth.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
